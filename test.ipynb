{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparser import argparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_dataloader, load_config\n",
    "args=argparser(args=[\"--config\", \"config/mninst_ach.json\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config file: config/mninst_ach.json\n"
     ]
    }
   ],
   "source": [
    "conf = load_config(args)\n",
    "train_data, test_data = config_dataloader(conf, **conf['training_params']['loader_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([256, 1, 28, 28])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_data)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_digit(image):\n",
    "    if len(image.shape) == 1:\n",
    "        side = int(image.shape[0]**0.5+0.0001)\n",
    "        image = image.view((side, side))\n",
    "    plt.imshow(image.numpy().squeeze(), cmap='gray_r');\n",
    "    plt.colorbar()\n",
    "    \n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer details for the neural network\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=20):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in train_data:\n",
    "            # Flatten MNIST images into a 784 long vector\n",
    "            images = images.view(images.shape[0], -1)\n",
    "\n",
    "            # Training pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            #This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            #And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.0507212273618007\n",
      "Epoch 1 - Training loss: 0.8412431401141146\n",
      "Epoch 2 - Training loss: 0.47881185399725085\n",
      "Epoch 3 - Training loss: 0.3923909460610532\n",
      "Epoch 4 - Training loss: 0.3529121033054717\n",
      "Epoch 5 - Training loss: 0.3272516823829488\n",
      "Epoch 6 - Training loss: 0.3075757210559033\n",
      "Epoch 7 - Training loss: 0.2908138465374074\n",
      "Epoch 8 - Training loss: 0.27531632286437013\n",
      "Epoch 9 - Training loss: 0.26159210541146866\n",
      "Epoch 10 - Training loss: 0.2491595353851927\n",
      "Epoch 11 - Training loss: 0.2375503621836926\n",
      "Epoch 12 - Training loss: 0.2266115924779405\n",
      "Epoch 13 - Training loss: 0.2171965781044453\n",
      "Epoch 14 - Training loss: 0.2081038087923476\n",
      "Epoch 15 - Training loss: 0.19964948474092686\n",
      "Epoch 16 - Training loss: 0.19199557849701415\n",
      "Epoch 17 - Training loss: 0.18492326574756743\n",
      "Epoch 18 - Training loss: 0.177860293997095\n",
      "Epoch 19 - Training loss: 0.17221451520602754\n"
     ]
    }
   ],
   "source": [
    "train(model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9614\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images,labels in test_data:\n",
    "    for i in range(len(images)):\n",
    "        img = images[i].view(1, 784)\n",
    "        # Turn off gradients to speed up this part\n",
    "        with torch.no_grad():\n",
    "            logps = model(img)\n",
    "\n",
    "        # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        pred_label = probab.index(max(probab))\n",
    "        true_label = labels.numpy()[i]\n",
    "        if(true_label == pred_label):\n",
    "            correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distribution of the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take model, datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(test, i):\n",
    "    iterator = iter(test_data)\n",
    "    for j in range(i):\n",
    "        iterator.next()\n",
    "    return iterator.next()\n",
    "\n",
    "image, label = get_item(test_data, 73)\n",
    "image = image.view((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_infinity_uniform_noise(data, epsilon, n_samples=1000):\n",
    "    shape = (n_samples,) + data.shape[1:]\n",
    "    noise = (torch.rand(shape) * 2 - 1) * epsilon\n",
    "    return torch.clamp(data + noise, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed = l_infinity_uniform_noise(image, 0.7, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(perturbed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5661)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model(perturbed).argmax(axis=1) == model(image).argmax()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.)\n",
      "1 tensor(1.)\n",
      "2 tensor(1.)\n",
      "3 tensor(1.)\n",
      "4 tensor(1.)\n",
      "5 tensor(1.)\n",
      "6 tensor(1.)\n",
      "7 tensor(0.9999)\n",
      "8 tensor(1.)\n",
      "9 tensor(0.9990)\n",
      "10 tensor(1.)\n",
      "11 tensor(1.)\n",
      "12 tensor(1.)\n",
      "13 tensor(1.)\n",
      "14 tensor(1.)\n",
      "15 tensor(1.)\n",
      "16 tensor(1.)\n",
      "17 tensor(1.)\n",
      "18 tensor(1.)\n",
      "19 tensor(1.)\n",
      "20 tensor(0.9991)\n",
      "21 tensor(1.)\n",
      "22 tensor(1.)\n",
      "23 tensor(1.)\n",
      "24 tensor(1.)\n",
      "25 tensor(1.)\n",
      "26 tensor(1.)\n",
      "27 tensor(1.)\n",
      "28 tensor(1.)\n",
      "29 tensor(1.)\n",
      "30 tensor(1.)\n",
      "31 tensor(0.9999)\n",
      "32 tensor(1.)\n",
      "33 tensor(0.9178)\n",
      "34 tensor(1.)\n",
      "35 tensor(1.)\n",
      "36 tensor(1.)\n",
      "37 tensor(1.)\n",
      "38 tensor(0.8681)\n",
      "39 tensor(1.)\n",
      "40 tensor(1.)\n",
      "41 tensor(1.)\n",
      "42 tensor(0.9993)\n",
      "43 tensor(1.)\n",
      "44 tensor(0.9999)\n",
      "45 tensor(1.)\n",
      "46 tensor(0.9995)\n",
      "47 tensor(1.)\n",
      "48 tensor(1.)\n",
      "49 tensor(1.)\n",
      "50 tensor(1.)\n",
      "51 tensor(1.)\n",
      "52 tensor(1.)\n",
      "53 tensor(1.)\n",
      "54 tensor(1.)\n",
      "55 tensor(1.)\n",
      "56 tensor(1.)\n",
      "57 tensor(1.)\n",
      "58 tensor(1.)\n",
      "59 tensor(0.9998)\n",
      "60 tensor(1.)\n",
      "61 tensor(0.9982)\n",
      "62 tensor(0.9005)\n",
      "63 tensor(0.7940)\n",
      "64 tensor(1.)\n",
      "65 tensor(0.9774)\n",
      "66 tensor(0.8062)\n",
      "67 tensor(1.)\n",
      "68 tensor(1.)\n",
      "69 tensor(1.)\n",
      "70 tensor(1.)\n",
      "71 tensor(1.)\n",
      "72 tensor(1.)\n",
      "73 tensor(0.5760)\n",
      "74 tensor(1.)\n",
      "75 tensor(1.)\n",
      "76 tensor(1.)\n",
      "77 tensor(0.9940)\n",
      "78 tensor(0.9993)\n",
      "79 tensor(1.)\n",
      "80 tensor(0.9953)\n",
      "81 tensor(1.)\n",
      "82 tensor(1.)\n",
      "83 tensor(1.)\n",
      "84 tensor(0.9999)\n",
      "85 tensor(1.)\n",
      "86 tensor(1.)\n",
      "87 tensor(1.)\n",
      "88 tensor(1.)\n",
      "89 tensor(1.)\n",
      "90 tensor(1.)\n",
      "91 tensor(1.)\n",
      "92 tensor(0.6684)\n",
      "93 tensor(1.)\n",
      "94 tensor(1.)\n",
      "95 tensor(1.)\n",
      "96 tensor(0.9796)\n",
      "97 tensor(1.)\n",
      "98 tensor(1.)\n",
      "99 tensor(1.)\n",
      "100 tensor(1.)\n",
      "101 tensor(1.)\n",
      "102 tensor(1.)\n",
      "103 tensor(1.)\n",
      "104 tensor(0.7561)\n",
      "105 tensor(1.)\n",
      "106 tensor(1.)\n",
      "107 tensor(0.9986)\n",
      "108 tensor(0.9996)\n",
      "109 tensor(1.)\n",
      "110 tensor(1.)\n",
      "111 tensor(1.)\n",
      "112 tensor(1.)\n",
      "113 tensor(1.)\n",
      "114 tensor(0.9978)\n",
      "115 tensor(0.9673)\n",
      "116 tensor(0.9974)\n",
      "117 tensor(1.)\n",
      "118 tensor(0.9883)\n",
      "119 tensor(0.9998)\n",
      "120 tensor(1.)\n",
      "121 tensor(0.9807)\n",
      "122 tensor(1.)\n",
      "123 tensor(1.)\n",
      "124 tensor(0.6140)\n",
      "125 tensor(0.9861)\n",
      "126 tensor(0.8882)\n",
      "127 tensor(1.)\n",
      "128 tensor(1.)\n",
      "129 tensor(1.)\n",
      "130 tensor(1.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-507cc5a4b880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mperturbed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_infinity_uniform_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturbed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-5922c974699f>\u001b[0m in \u001b[0;36ml_infinity_uniform_noise\u001b[0;34m(data, epsilon, n_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0ml_infinity_uniform_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for data, target in test_data:\n",
    "    data = data.view(images.shape[0], -1)\n",
    "    perturbed = l_infinity_uniform_noise(data, 0.7, 10000)\n",
    "    print(i,(model(perturbed).argmax(axis=1) == model(data).argmax()).float().mean())\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM attack code\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [-1,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, -1, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epsilon ):\n",
    "\n",
    "    # Accuracy counter\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    # Loop over all examples in test set\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(images.shape[0], -1)\n",
    "        # Set requires_grad attribute of tensor. Important for Attack\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # Forward pass the data through the model\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        # If the initial prediction is wrong, dont bother attacking, just move on\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = nn.NLLLoss()(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        # Call FGSM Attack\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        # Check for success\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            # Special case for saving 0 epsilon examples\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            # Save some adv examples for visualization later\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    # Return the accuracy and an adversarial example\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0\tTest Accuracy = 9614 / 10000 = 0.9614\n",
      "Epsilon: 0.001\tTest Accuracy = 9609 / 10000 = 0.9609\n",
      "Epsilon: 0.01\tTest Accuracy = 9520 / 10000 = 0.952\n",
      "Epsilon: 0.1\tTest Accuracy = 7309 / 10000 = 0.7309\n",
      "Epsilon: 0.2\tTest Accuracy = 2353 / 10000 = 0.2353\n",
      "Epsilon: 0.3\tTest Accuracy = 393 / 10000 = 0.0393\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "epsilons =[0, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, test_data, eps);\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take model, take sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def logprob(x, ivar):\n",
    "    logp = -0.5 * np.sum(ivar * x**2)\n",
    "    grad = -ivar * x\n",
    "    return logp, grad\n",
    "# run the sampler\n",
    "from pyhmc import hmc\n",
    "ivar = 1. / np.random.rand(5)\n",
    "samples = hmc(logprob, x0=np.random.randn(5), args=(ivar,), n_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(x) = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified robust "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading config file: config/mninst_ach.json\n",
      "./mnist_crown/regular_dense.pth\n",
      "Command line: /opt/miniconda3/envs/scvi/lib/python3.7/site-packages/ipykernel_launcher.py -f /Users/achille/Library/Jupyter/runtime/kernel-b30aaf25-e62a-4088-aa6b-b3d608f7a8b2.json\n",
      "training configurations: {'method': 'robust', 'epochs': 100, 'lr': 0.0005, 'lr_decay_step': 10, 'lr_decay_factor': 0.5, 'lr_decay_milestones': [], 'weight_decay': 0.0, 'optimizer': 'adam', 'starting_epsilon': 0.0, 'epsilon': 0.3, 'schedule_length': 61, 'schedule_type': 'linear', 'norm': 'inf', 'verbose': False, 'multi_gpu': False, 'loader_params': {'batch_size': 256, 'shuffle_train': True, 'test_batch_size': 256}, 'method_params': {'batch_size': 128, 'shuffle_train': True, 'runnerup_only': False, 'activity_reg': 0.0, 'l1_reg': 0.0, 'final-beta': 0.0, 'final-kappa': 0.5, 'convex-proj': 50, 'bound_opts': {'same-slope': False, 'zero-lb': False, 'one-lb': False}, 'batch_multiplier': 1, 'bounded_input': True, 'bound_type': 'crown-interval'}, 'schedule_start': 1}\n",
      "Model structure:\n",
      "BoundSequential(\n",
      "  (0): BoundFlatten()\n",
      "  (1): BoundLinear(in_features=784, out_features=128, bias=True)\n",
      "  (2): BoundReLU()\n",
      "  (3): BoundLinear(in_features=128, out_features=64, bias=True)\n",
      "  (4): BoundReLU()\n",
      "  (5): BoundLinear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "data std: [1.0]\n",
      "Epoch 0, learning rate [0.0005], epsilon 0 - 0\n",
      "eps 0.0 close to 0, using natural training\n",
      "[ 0:   0]: eps 0.000000  Time 0.069 (0.069)  Total Loss 2.3117 (2.3117)  L1 Loss 0.0000 (0.0000)  CE 2.3117 (2.3117)  RCE 0.0000 (0.0000)  Err 0.9219 (0.9219)  Rob Err 0.0000 (0.0000)  Uns 0.0 (0.0)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tightness 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  R 0.291  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[ 0:  50]: eps 0.000000  Time 0.004 (0.006)  Total Loss 1.0298 (1.7599)  L1 Loss 0.0000 (0.0000)  CE 1.0298 (1.7599)  RCE 0.0000 (0.0000)  Err 0.2305 (0.4685)  Rob Err 0.0000 (0.0000)  Uns 0.0 (0.0)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tightness 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  R 11.225  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[ 0: 100]: eps 0.000000  Time 0.004 (0.005)  Total Loss 0.4627 (1.2250)  L1 Loss 0.0000 (0.0000)  CE 0.4627 (1.2250)  RCE 0.0000 (0.0000)  Err 0.1250 (0.3202)  Rob Err 0.0000 (0.0000)  Uns 0.0 (0.0)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tightness 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  R 24.015  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[ 0: 150]: eps 0.000000  Time 0.005 (0.005)  Total Loss 0.4162 (0.9617)  L1 Loss 0.0000 (0.0000)  CE 0.4162 (0.9617)  RCE 0.0000 (0.0000)  Err 0.1289 (0.2528)  Rob Err 0.0000 (0.0000)  Uns 0.0 (0.0)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tightness 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  R 31.475  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[ 0: 200]: eps 0.000000  Time 0.004 (0.005)  Total Loss 0.3072 (0.8119)  L1 Loss 0.0000 (0.0000)  CE 0.3072 (0.8119)  RCE 0.0000 (0.0000)  Err 0.0781 (0.2149)  Rob Err 0.0000 (0.0000)  Uns 0.0 (0.0)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tightness 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  R 29.894  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 0 eps:0.0000]: Time 0.003 (0.005)  Total Loss 0.4416 (0.7442)  L1 Loss 0.0000 (0.0000)  CE 0.4416 (0.7442)  RCE 0.0000 (0.0000)  Uns 0.000 (0.000)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tight 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  Err 0.1250 (0.1980)  Rob Err 0.0000 (0.0000)  R 28.124  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 25.921405792236328\n",
      "layer 3 norm 8.359037399291992\n",
      "layer 5 norm 6.417342185974121\n",
      "Epoch time: 3.2604, Total time: 3.2604\n",
      "Evaluating...\n",
      "eps 0.0 close to 0, using natural training\n",
      "[FINAL RESULT epoch: 0 eps:0.0000]: Time 0.001 (0.003)  Total Loss 0.1590 (0.3106)  L1 Loss 0.0000 (0.0000)  CE 0.1590 (0.3106)  RCE 0.0000 (0.0000)  Uns 0.000 (0.000)  Dead 0.0 (0.0)  Alive 0.0 (0.0)  Tight 0.00000 (0.00000)  Bias 0.00000 (0.00000)  Diff 0.00000 (0.00000)  Err 0.0625 (0.0874)  Rob Err 0.0000 (0.0000)  R 27.426  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 25.921405792236328\n",
      "layer 3 norm 8.359037399291992\n",
      "layer 5 norm 6.417342185974121\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 1, learning rate [0.0005], epsilon 0 - 0.005\n",
      "[ 1:   0]: eps 0.000000  Time 0.079 (0.079)  Total Loss 0.3137 (0.3137)  L1 Loss 0.0000 (0.0000)  CE 0.3137 (0.3137)  RCE 0.3137 (0.3137)  Err 0.1055 (0.1055)  Rob Err 0.1055 (0.1055)  Uns 0.0 (0.0)  Dead 26.1 (26.1)  Alive 165.9 (165.9)  Tightness 0.00000 (0.00000)  Bias 0.35279 (0.35279)  Diff 0.00000 (0.00000)  R 30.045  beta 1.000 (1.000)  kappa 1.000 (1.000)  \n",
      "[ 1:  50]: eps 0.001064  Time 0.033 (0.038)  Total Loss 0.3491 (0.3277)  L1 Loss 0.0000 (0.0000)  CE 0.3383 (0.3224)  RCE 0.3491 (0.3277)  Err 0.1055 (0.0914)  Rob Err 0.1172 (0.0933)  Uns 2.4 (1.0)  Dead 27.4 (26.7)  Alive 162.2 (164.3)  Tightness 2.35547 (1.02091)  Bias 0.54656 (0.47824)  Diff 5.18551 (2.60388)  R 36.380  beta 0.996 (0.996)  kappa 1.000 (1.000)  \n",
      "[ 1: 100]: eps 0.002128  Time 0.036 (0.037)  Total Loss 0.4103 (0.3165)  L1 Loss 0.0000 (0.0000)  CE 0.3848 (0.3061)  RCE 0.4103 (0.3165)  Err 0.0938 (0.0860)  Rob Err 0.1055 (0.0898)  Uns 3.7 (2.0)  Dead 25.9 (26.6)  Alive 162.4 (163.4)  Tightness 3.66016 (2.00197)  Bias 0.39588 (0.45746)  Diff 10.56723 (5.23306)  R 32.791  beta 0.993 (0.993)  kappa 1.000 (1.000)  \n",
      "[ 1: 150]: eps 0.003191  Time 0.032 (0.036)  Total Loss 0.3504 (0.3126)  L1 Loss 0.0000 (0.0000)  CE 0.3150 (0.2967)  RCE 0.3504 (0.3126)  Err 0.1016 (0.0841)  Rob Err 0.1016 (0.0893)  Uns 5.8 (2.9)  Dead 27.9 (26.8)  Alive 158.4 (162.3)  Tightness 5.75391 (2.90879)  Bias 0.40470 (0.41018)  Diff 15.33088 (7.80670)  R 36.024  beta 0.989 (0.989)  kappa 1.000 (1.000)  \n",
      "[ 1: 200]: eps 0.004255  Time 0.037 (0.036)  Total Loss 0.2782 (0.3103)  L1 Loss 0.0000 (0.0000)  CE 0.2381 (0.2887)  RCE 0.2782 (0.3103)  Err 0.0664 (0.0820)  Rob Err 0.0781 (0.0886)  Uns 7.0 (3.7)  Dead 28.1 (27.0)  Alive 157.0 (161.2)  Tightness 6.97656 (3.74623)  Bias -0.19400 (0.34558)  Diff 20.45350 (10.33405)  R 35.032  beta 0.986 (0.986)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 1 eps:0.0050]: Time 0.016 (0.036)  Total Loss 0.4492 (0.3113)  L1 Loss 0.0000 (0.0000)  CE 0.3821 (0.2858)  RCE 0.4492 (0.3113)  Uns 7.583 (4.283)  Dead 27.8 (27.1)  Alive 156.6 (160.6)  Tight 7.58333 (4.28263)  Bias -0.33926 (0.28617)  Diff 23.62199 (12.00146)  Err 0.1042 (0.0809)  Rob Err 0.1250 (0.0882)  R 31.455  beta 0.983 (0.983)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 29.892953872680664\n",
      "layer 3 norm 8.993010520935059\n",
      "layer 5 norm 6.854837894439697\n",
      "Epoch time: 8.5387, Total time: 11.7992\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 1 eps:0.0050]: Time 0.003 (0.016)  Total Loss 0.1106 (0.3010)  L1 Loss 0.0000 (0.0000)  CE 0.0837 (0.2483)  RCE 0.1106 (0.3010)  Uns 5.750 (7.725)  Dead 24.5 (28.0)  Alive 161.8 (156.3)  Tight 5.75000 (7.72460)  Bias 0.31645 (-0.11205)  Diff 25.36456 (23.79383)  Err 0.0625 (0.0701)  Rob Err 0.0625 (0.0864)  R 29.274  beta 0.983 (0.983)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 29.892953872680664\n",
      "layer 3 norm 8.993010520935059\n",
      "layer 5 norm 6.854837894439697\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 2, learning rate [0.0005], epsilon 0.005 - 0.01\n",
      "[ 2:   0]: eps 0.005000  Time 0.098 (0.098)  Total Loss 0.2308 (0.2308)  L1 Loss 0.0000 (0.0000)  CE 0.1854 (0.1854)  RCE 0.2308 (0.2308)  Err 0.0664 (0.0664)  Rob Err 0.0781 (0.0781)  Uns 8.2 (8.2)  Dead 28.2 (28.2)  Alive 155.6 (155.6)  Tightness 8.16016 (8.16016)  Bias -0.16893 (-0.16893)  Diff 23.62898 (23.62898)  R 35.789  beta 0.983 (0.983)  kappa 1.000 (1.000)  \n",
      "[ 2:  50]: eps 0.006064  Time 0.037 (0.041)  Total Loss 0.3116 (0.3162)  L1 Loss 0.0000 (0.0000)  CE 0.2545 (0.2569)  RCE 0.3116 (0.3162)  Err 0.0742 (0.0738)  Rob Err 0.0781 (0.0900)  Uns 8.9 (8.5)  Dead 29.4 (28.4)  Alive 153.7 (155.0)  Tightness 8.91797 (8.53140)  Bias -0.17497 (-0.24181)  Diff 28.10650 (26.05200)  R 37.092  beta 0.980 (0.980)  kappa 1.000 (1.000)  \n",
      "[ 2: 100]: eps 0.007128  Time 0.033 (0.038)  Total Loss 0.3245 (0.3048)  L1 Loss 0.0000 (0.0000)  CE 0.2545 (0.2413)  RCE 0.3245 (0.3048)  Err 0.0742 (0.0699)  Rob Err 0.0898 (0.0876)  Uns 10.0 (9.1)  Dead 29.1 (28.9)  Alive 153.0 (154.0)  Tightness 9.97266 (9.10655)  Bias -0.60512 (-0.35186)  Diff 33.07214 (28.25625)  R 32.565  beta 0.976 (0.976)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2: 150]: eps 0.008191  Time 0.034 (0.037)  Total Loss 0.2880 (0.3082)  L1 Loss 0.0000 (0.0000)  CE 0.2127 (0.2389)  RCE 0.2880 (0.3082)  Err 0.0391 (0.0684)  Rob Err 0.0664 (0.0880)  Uns 10.9 (9.6)  Dead 29.0 (28.9)  Alive 152.1 (153.5)  Tightness 10.92188 (9.58004)  Bias -0.56856 (-0.46729)  Diff 37.66497 (30.62593)  R 40.722  beta 0.973 (0.973)  kappa 1.000 (1.000)  \n",
      "[ 2: 200]: eps 0.009255  Time 0.034 (0.036)  Total Loss 0.2338 (0.3115)  L1 Loss 0.0000 (0.0000)  CE 0.1501 (0.2364)  RCE 0.2338 (0.3115)  Err 0.0430 (0.0677)  Rob Err 0.0664 (0.0890)  Uns 11.9 (10.0)  Dead 30.3 (29.1)  Alive 149.8 (152.9)  Tightness 11.92188 (9.99201)  Bias -0.95675 (-0.57513)  Diff 41.31619 (32.82769)  R 34.971  beta 0.969 (0.969)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 2 eps:0.0100]: Time 0.017 (0.036)  Total Loss 0.3730 (0.3104)  L1 Loss 0.0000 (0.0000)  CE 0.2866 (0.2321)  RCE 0.3730 (0.3104)  Uns 12.021 (10.243)  Dead 30.1 (29.3)  Alive 149.9 (152.4)  Tight 12.02083 (10.24343)  Bias -0.83066 (-0.63933)  Diff 44.78639 (34.23733)  Err 0.0417 (0.0664)  Rob Err 0.0729 (0.0885)  R 29.660  beta 0.967 (0.967)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 33.87841796875\n",
      "layer 3 norm 9.25370979309082\n",
      "layer 5 norm 7.052945613861084\n",
      "Epoch time: 8.5504, Total time: 20.3495\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 2 eps:0.0100]: Time 0.003 (0.017)  Total Loss 0.1164 (0.3024)  L1 Loss 0.0000 (0.0000)  CE 0.0682 (0.2035)  RCE 0.1164 (0.3024)  Uns 7.562 (11.606)  Dead 29.2 (30.1)  Alive 155.2 (150.3)  Tight 7.56250 (11.60570)  Bias -0.07805 (-1.02694)  Diff 46.07933 (44.57016)  Err 0.0625 (0.0596)  Rob Err 0.0625 (0.0877)  R 29.107  beta 0.967 (0.967)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 33.87841796875\n",
      "layer 3 norm 9.25370979309082\n",
      "layer 5 norm 7.052945613861084\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 3, learning rate [0.0005], epsilon 0.01 - 0.015\n",
      "[ 3:   0]: eps 0.010000  Time 0.104 (0.104)  Total Loss 0.3619 (0.3619)  L1 Loss 0.0000 (0.0000)  CE 0.2548 (0.2548)  RCE 0.3619 (0.3619)  Err 0.0547 (0.0547)  Rob Err 0.0977 (0.0977)  Uns 11.8 (11.8)  Dead 30.7 (30.7)  Alive 149.6 (149.6)  Tightness 11.77734 (11.77734)  Bias -1.04576 (-1.04576)  Diff 44.23340 (44.23340)  R 35.420  beta 0.967 (0.967)  kappa 1.000 (1.000)  \n",
      "[ 3:  50]: eps 0.011064  Time 0.032 (0.040)  Total Loss 0.2482 (0.3085)  L1 Loss 0.0000 (0.0000)  CE 0.1546 (0.2011)  RCE 0.2482 (0.3085)  Err 0.0547 (0.0574)  Rob Err 0.0742 (0.0881)  Uns 12.1 (12.0)  Dead 30.5 (30.3)  Alive 149.3 (149.7)  Tightness 12.12891 (11.99104)  Bias -1.25413 (-1.15303)  Diff 48.59556 (46.55254)  R 34.608  beta 0.963 (0.963)  kappa 1.000 (1.000)  \n",
      "[ 3: 100]: eps 0.012128  Time 0.035 (0.037)  Total Loss 0.3410 (0.3159)  L1 Loss 0.0000 (0.0000)  CE 0.2084 (0.2026)  RCE 0.3410 (0.3159)  Err 0.0742 (0.0583)  Rob Err 0.1172 (0.0908)  Uns 12.4 (12.3)  Dead 31.1 (30.5)  Alive 148.5 (149.2)  Tightness 12.36328 (12.26423)  Bias -1.07485 (-1.20846)  Diff 52.28134 (48.48745)  R 31.133  beta 0.960 (0.960)  kappa 1.000 (1.000)  \n",
      "[ 3: 150]: eps 0.013191  Time 0.032 (0.036)  Total Loss 0.2751 (0.3182)  L1 Loss 0.0000 (0.0000)  CE 0.1582 (0.2005)  RCE 0.2751 (0.3182)  Err 0.0469 (0.0575)  Rob Err 0.0742 (0.0914)  Uns 13.2 (12.5)  Dead 32.3 (30.8)  Alive 146.6 (148.7)  Tightness 13.17578 (12.50476)  Bias -1.36921 (-1.26824)  Diff 55.13468 (50.24874)  R 33.256  beta 0.956 (0.956)  kappa 1.000 (1.000)  \n",
      "[ 3: 200]: eps 0.014255  Time 0.034 (0.035)  Total Loss 0.3508 (0.3224)  L1 Loss 0.0000 (0.0000)  CE 0.1908 (0.1992)  RCE 0.3508 (0.3224)  Err 0.0703 (0.0570)  Rob Err 0.0977 (0.0933)  Uns 13.0 (12.7)  Dead 31.5 (31.0)  Alive 147.5 (148.3)  Tightness 12.99609 (12.66229)  Bias -1.36130 (-1.29110)  Diff 58.68736 (51.96357)  R 30.303  beta 0.952 (0.952)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 3 eps:0.0150]: Time 0.018 (0.035)  Total Loss 0.2837 (0.3224)  L1 Loss 0.0000 (0.0000)  CE 0.1398 (0.1969)  RCE 0.2837 (0.3224)  Uns 14.177 (12.737)  Dead 32.9 (31.2)  Alive 144.9 (148.1)  Tight 14.17708 (12.73658)  Bias -1.27240 (-1.29906)  Diff 59.95672 (53.05259)  Err 0.0417 (0.0561)  Rob Err 0.0938 (0.0930)  R 32.716  beta 0.950 (0.950)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 35.936397552490234\n",
      "layer 3 norm 9.31014347076416\n",
      "layer 5 norm 7.237841606140137\n",
      "Epoch time: 8.4907, Total time: 28.8402\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 3 eps:0.0150]: Time 0.003 (0.016)  Total Loss 0.1640 (0.3181)  L1 Loss 0.0000 (0.0000)  CE 0.0777 (0.1764)  RCE 0.1640 (0.3181)  Uns 10.000 (13.087)  Dead 31.0 (32.3)  Alive 151.0 (146.6)  Tight 10.00000 (13.08680)  Bias -0.27466 (-1.35649)  Diff 62.11992 (60.76720)  Err 0.0625 (0.0527)  Rob Err 0.1250 (0.0925)  R 29.048  beta 0.950 (0.950)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 35.936397552490234\n",
      "layer 3 norm 9.31014347076416\n",
      "layer 5 norm 7.237841606140137\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 4, learning rate [0.0005], epsilon 0.015 - 0.02\n",
      "[ 4:   0]: eps 0.015000  Time 0.105 (0.105)  Total Loss 0.3325 (0.3325)  L1 Loss 0.0000 (0.0000)  CE 0.1734 (0.1734)  RCE 0.3325 (0.3325)  Err 0.0508 (0.0508)  Rob Err 0.1055 (0.1055)  Uns 13.4 (13.4)  Dead 32.5 (32.5)  Alive 146.1 (146.1)  Tightness 13.39453 (13.39453)  Bias -1.09471 (-1.09471)  Diff 60.76787 (60.76787)  R 33.716  beta 0.950 (0.950)  kappa 1.000 (1.000)  \n",
      "[ 4:  50]: eps 0.016064  Time 0.031 (0.039)  Total Loss 0.2986 (0.3229)  L1 Loss 0.0000 (0.0000)  CE 0.1502 (0.1736)  RCE 0.2986 (0.3229)  Err 0.0469 (0.0495)  Rob Err 0.1016 (0.0938)  Uns 13.9 (13.4)  Dead 33.0 (32.3)  Alive 145.0 (146.2)  Tightness 13.94141 (13.41904)  Bias -1.50312 (-1.42699)  Diff 63.36749 (62.32475)  R 35.624  beta 0.946 (0.946)  kappa 1.000 (1.000)  \n",
      "[ 4: 100]: eps 0.017128  Time 0.033 (0.036)  Total Loss 0.2773 (0.3361)  L1 Loss 0.0000 (0.0000)  CE 0.1332 (0.1785)  RCE 0.2773 (0.3361)  Err 0.0273 (0.0517)  Rob Err 0.0742 (0.0977)  Uns 13.0 (13.5)  Dead 33.7 (32.8)  Alive 145.3 (145.7)  Tightness 13.01562 (13.52754)  Bias -1.49558 (-1.43902)  Diff 65.47511 (63.39025)  R 32.657  beta 0.943 (0.943)  kappa 1.000 (1.000)  \n",
      "[ 4: 150]: eps 0.018191  Time 0.032 (0.035)  Total Loss 0.3650 (0.3390)  L1 Loss 0.0000 (0.0000)  CE 0.1881 (0.1770)  RCE 0.3650 (0.3390)  Err 0.0508 (0.0513)  Rob Err 0.0820 (0.0982)  Uns 13.9 (13.6)  Dead 34.3 (33.1)  Alive 143.8 (145.3)  Tightness 13.89062 (13.63261)  Bias -1.00423 (-1.43233)  Diff 67.39965 (64.54266)  R 33.697  beta 0.939 (0.939)  kappa 1.000 (1.000)  \n",
      "[ 4: 200]: eps 0.019255  Time 0.031 (0.035)  Total Loss 0.3854 (0.3430)  L1 Loss 0.0000 (0.0000)  CE 0.1822 (0.1760)  RCE 0.3854 (0.3430)  Err 0.0547 (0.0512)  Rob Err 0.1094 (0.1000)  Uns 13.8 (13.8)  Dead 33.4 (33.3)  Alive 144.9 (144.9)  Tightness 13.75781 (13.75738)  Bias -1.63499 (-1.45391)  Diff 70.73215 (65.61269)  R 32.992  beta 0.936 (0.936)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 4 eps:0.0200]: Time 0.020 (0.035)  Total Loss 0.4012 (0.3452)  L1 Loss 0.0000 (0.0000)  CE 0.1987 (0.1746)  RCE 0.4012 (0.3452)  Uns 15.260 (13.862)  Dead 34.4 (33.5)  Alive 142.3 (144.7)  Tight 15.26042 (13.86167)  Bias -2.53250 (-1.47955)  Diff 70.79673 (66.30212)  Err 0.0521 (0.0505)  Rob Err 0.1146 (0.1007)  R 28.663  beta 0.933 (0.933)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 35.47599411010742\n",
      "layer 3 norm 9.30955982208252\n",
      "layer 5 norm 7.379992485046387\n",
      "Epoch time: 8.3470, Total time: 37.1872\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 4 eps:0.0200]: Time 0.003 (0.016)  Total Loss 0.2032 (0.3464)  L1 Loss 0.0000 (0.0000)  CE 0.0818 (0.1586)  RCE 0.2032 (0.3464)  Uns 10.375 (14.329)  Dead 33.2 (34.4)  Alive 148.4 (143.3)  Tight 10.37500 (14.32850)  Bias -0.39233 (-1.54851)  Diff 72.27660 (71.07615)  Err 0.0625 (0.0463)  Rob Err 0.0625 (0.1024)  R 29.305  beta 0.933 (0.933)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 35.47599411010742\n",
      "layer 3 norm 9.30955982208252\n",
      "layer 5 norm 7.379992485046387\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 5, learning rate [0.0005], epsilon 0.02 - 0.025\n",
      "[ 5:   0]: eps 0.020000  Time 0.086 (0.086)  Total Loss 0.3596 (0.3596)  L1 Loss 0.0000 (0.0000)  CE 0.1531 (0.1531)  RCE 0.3596 (0.3596)  Err 0.0547 (0.0547)  Rob Err 0.1328 (0.1328)  Uns 14.9 (14.9)  Dead 34.7 (34.7)  Alive 142.4 (142.4)  Tightness 14.91406 (14.91406)  Bias -1.61272 (-1.61272)  Diff 70.71256 (70.71256)  R 36.068  beta 0.933 (0.933)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5:  50]: eps 0.021064  Time 0.031 (0.039)  Total Loss 0.3256 (0.3682)  L1 Loss 0.0000 (0.0000)  CE 0.1323 (0.1661)  RCE 0.3256 (0.3682)  Err 0.0508 (0.0491)  Rob Err 0.0977 (0.1060)  Uns 15.0 (14.8)  Dead 35.3 (34.7)  Alive 141.6 (142.5)  Tightness 15.01172 (14.78784)  Bias -1.66080 (-1.67415)  Diff 72.23446 (71.67559)  R 31.482  beta 0.930 (0.930)  kappa 1.000 (1.000)  \n",
      "[ 5: 100]: eps 0.022128  Time 0.033 (0.036)  Total Loss 0.3313 (0.3770)  L1 Loss 0.0000 (0.0000)  CE 0.1338 (0.1682)  RCE 0.3313 (0.3770)  Err 0.0430 (0.0489)  Rob Err 0.0898 (0.1093)  Uns 15.3 (14.9)  Dead 35.6 (35.0)  Alive 141.2 (142.1)  Tightness 15.29297 (14.86564)  Bias -1.69881 (-1.66890)  Diff 73.23051 (72.21384)  R 32.309  beta 0.926 (0.926)  kappa 1.000 (1.000)  \n",
      "[ 5: 150]: eps 0.023191  Time 0.032 (0.036)  Total Loss 0.3586 (0.3765)  L1 Loss 0.0000 (0.0000)  CE 0.1347 (0.1646)  RCE 0.3586 (0.3765)  Err 0.0391 (0.0480)  Rob Err 0.1211 (0.1091)  Uns 15.6 (15.1)  Dead 36.2 (35.3)  Alive 140.2 (141.6)  Tightness 15.60156 (15.05924)  Bias -2.00771 (-1.71423)  Diff 73.66409 (72.67217)  R 30.101  beta 0.923 (0.923)  kappa 1.000 (1.000)  \n",
      "[ 5: 200]: eps 0.024255  Time 0.034 (0.035)  Total Loss 0.4457 (0.3780)  L1 Loss 0.0000 (0.0000)  CE 0.1880 (0.1628)  RCE 0.4457 (0.3780)  Err 0.0703 (0.0474)  Rob Err 0.1328 (0.1090)  Uns 15.7 (15.2)  Dead 37.6 (35.5)  Alive 138.7 (141.3)  Tightness 15.72656 (15.21617)  Bias -2.08592 (-1.76117)  Diff 72.95574 (73.14072)  R 32.289  beta 0.919 (0.919)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 5 eps:0.0250]: Time 0.017 (0.035)  Total Loss 0.1824 (0.3790)  L1 Loss 0.0000 (0.0000)  CE 0.0399 (0.1610)  RCE 0.1824 (0.3790)  Uns 16.135 (15.308)  Dead 37.2 (35.7)  Alive 138.7 (141.0)  Tight 16.13542 (15.30763)  Bias -2.14846 (-1.78714)  Diff 73.81621 (73.36997)  Err 0.0000 (0.0467)  Rob Err 0.0312 (0.1098)  R 31.731  beta 0.917 (0.917)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 33.529762268066406\n",
      "layer 3 norm 9.30428695678711\n",
      "layer 5 norm 7.4563093185424805\n",
      "Epoch time: 8.4076, Total time: 45.5948\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 5 eps:0.0250]: Time 0.003 (0.017)  Total Loss 0.2246 (0.3877)  L1 Loss 0.0000 (0.0000)  CE 0.0700 (0.1516)  RCE 0.2246 (0.3877)  Uns 12.250 (15.608)  Dead 35.2 (36.8)  Alive 144.5 (139.6)  Tight 12.25000 (15.60780)  Bias -0.71646 (-1.82785)  Diff 76.32088 (74.72769)  Err 0.0625 (0.0470)  Rob Err 0.0625 (0.1140)  R 28.923  beta 0.917 (0.917)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 33.529762268066406\n",
      "layer 3 norm 9.30428695678711\n",
      "layer 5 norm 7.4563093185424805\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 6, learning rate [0.0005], epsilon 0.025 - 0.03\n",
      "[ 6:   0]: eps 0.025000  Time 0.099 (0.099)  Total Loss 0.3432 (0.3432)  L1 Loss 0.0000 (0.0000)  CE 0.1337 (0.1337)  RCE 0.3432 (0.3432)  Err 0.0273 (0.0273)  Rob Err 0.0898 (0.0898)  Uns 16.3 (16.3)  Dead 37.0 (37.0)  Alive 138.7 (138.7)  Tightness 16.26172 (16.26172)  Bias -1.94122 (-1.94122)  Diff 74.61091 (74.61091)  R 31.355  beta 0.917 (0.917)  kappa 1.000 (1.000)  \n",
      "[ 6:  50]: eps 0.026064  Time 0.031 (0.040)  Total Loss 0.4383 (0.4024)  L1 Loss 0.0000 (0.0000)  CE 0.1535 (0.1543)  RCE 0.4383 (0.4024)  Err 0.0508 (0.0464)  Rob Err 0.1172 (0.1196)  Uns 16.2 (16.2)  Dead 37.5 (37.1)  Alive 138.3 (138.7)  Tightness 16.19922 (16.20757)  Bias -1.98874 (-2.01071)  Diff 75.28952 (74.89296)  R 33.867  beta 0.913 (0.913)  kappa 1.000 (1.000)  \n",
      "[ 6: 100]: eps 0.027128  Time 0.032 (0.037)  Total Loss 0.3754 (0.4075)  L1 Loss 0.0000 (0.0000)  CE 0.1209 (0.1546)  RCE 0.3754 (0.4075)  Err 0.0312 (0.0457)  Rob Err 0.1172 (0.1209)  Uns 17.4 (16.4)  Dead 38.7 (37.3)  Alive 135.9 (138.3)  Tightness 17.43359 (16.40756)  Bias -2.03324 (-2.05136)  Diff 73.44498 (74.94714)  R 31.020  beta 0.910 (0.910)  kappa 1.000 (1.000)  \n",
      "[ 6: 150]: eps 0.028191  Time 0.034 (0.036)  Total Loss 0.4391 (0.4135)  L1 Loss 0.0000 (0.0000)  CE 0.1351 (0.1559)  RCE 0.4391 (0.4135)  Err 0.0469 (0.0458)  Rob Err 0.1484 (0.1215)  Uns 16.4 (16.6)  Dead 38.2 (37.5)  Alive 137.4 (137.9)  Tightness 16.38672 (16.58353)  Bias -2.30730 (-2.11422)  Diff 74.34715 (74.79346)  R 32.826  beta 0.906 (0.906)  kappa 1.000 (1.000)  \n",
      "[ 6: 200]: eps 0.029255  Time 0.034 (0.035)  Total Loss 0.5238 (0.4165)  L1 Loss 0.0000 (0.0000)  CE 0.2102 (0.1544)  RCE 0.5238 (0.4165)  Err 0.0547 (0.0451)  Rob Err 0.1367 (0.1226)  Uns 16.8 (16.7)  Dead 38.8 (37.7)  Alive 136.4 (137.6)  Tightness 16.77734 (16.70208)  Bias -1.98682 (-2.12604)  Diff 73.80323 (74.68054)  R 29.725  beta 0.902 (0.902)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 6 eps:0.0300]: Time 0.017 (0.035)  Total Loss 0.3855 (0.4177)  L1 Loss 0.0000 (0.0000)  CE 0.1166 (0.1530)  RCE 0.3855 (0.4177)  Uns 16.667 (16.803)  Dead 38.2 (37.8)  Alive 137.1 (137.4)  Tight 16.66667 (16.80338)  Bias -2.23352 (-2.15066)  Diff 74.11983 (74.58312)  Err 0.0417 (0.0449)  Rob Err 0.1354 (0.1229)  R 32.673  beta 0.900 (0.900)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 31.21935272216797\n",
      "layer 3 norm 9.271432876586914\n",
      "layer 5 norm 7.519010543823242\n",
      "Epoch time: 8.4687, Total time: 54.0636\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 6 eps:0.0300]: Time 0.003 (0.017)  Total Loss 0.2272 (0.4279)  L1 Loss 0.0000 (0.0000)  CE 0.0523 (0.1437)  RCE 0.2272 (0.4279)  Uns 12.750 (17.089)  Dead 37.9 (38.8)  Alive 141.4 (136.1)  Tight 12.75000 (17.08860)  Bias -1.01076 (-2.28667)  Diff 75.50908 (73.80696)  Err 0.0000 (0.0433)  Rob Err 0.0625 (0.1287)  R 28.356  beta 0.900 (0.900)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 31.21935272216797\n",
      "layer 3 norm 9.271432876586914\n",
      "layer 5 norm 7.519010543823242\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 7, learning rate [0.0005], epsilon 0.03 - 0.035\n",
      "[ 7:   0]: eps 0.030000  Time 0.093 (0.093)  Total Loss 0.4199 (0.4199)  L1 Loss 0.0000 (0.0000)  CE 0.1217 (0.1217)  RCE 0.4199 (0.4199)  Err 0.0430 (0.0430)  Rob Err 0.1289 (0.1289)  Uns 17.0 (17.0)  Dead 39.1 (39.1)  Alive 135.9 (135.9)  Tightness 17.04688 (17.04688)  Bias -2.38577 (-2.38577)  Diff 73.08525 (73.08525)  R 31.189  beta 0.900 (0.900)  kappa 1.000 (1.000)  \n",
      "[ 7:  50]: eps 0.031064  Time 0.041 (0.041)  Total Loss 0.5279 (0.4269)  L1 Loss 0.0000 (0.0000)  CE 0.1550 (0.1400)  RCE 0.5279 (0.4269)  Err 0.0508 (0.0422)  Rob Err 0.1758 (0.1246)  Uns 18.8 (17.7)  Dead 38.0 (39.0)  Alive 135.2 (135.3)  Tightness 18.79297 (17.72158)  Bias -2.37553 (-2.42598)  Diff 74.54108 (73.67473)  R 28.499  beta 0.896 (0.896)  kappa 1.000 (1.000)  \n",
      "[ 7: 100]: eps 0.032128  Time 0.033 (0.038)  Total Loss 0.4603 (0.4371)  L1 Loss 0.0000 (0.0000)  CE 0.1476 (0.1429)  RCE 0.4603 (0.4371)  Err 0.0469 (0.0423)  Rob Err 0.1406 (0.1291)  Uns 18.5 (18.0)  Dead 40.1 (39.3)  Alive 133.3 (134.7)  Tightness 18.51562 (18.00665)  Bias -2.65083 (-2.50918)  Diff 71.81207 (73.40918)  R 31.312  beta 0.893 (0.893)  kappa 1.000 (1.000)  \n",
      "[ 7: 150]: eps 0.033191  Time 0.035 (0.037)  Total Loss 0.4271 (0.4465)  L1 Loss 0.0000 (0.0000)  CE 0.1350 (0.1460)  RCE 0.4271 (0.4465)  Err 0.0430 (0.0433)  Rob Err 0.1172 (0.1319)  Uns 18.1 (18.2)  Dead 41.0 (39.6)  Alive 132.8 (134.2)  Tightness 18.14844 (18.16238)  Bias -3.02356 (-2.51450)  Diff 69.74255 (72.69481)  R 35.515  beta 0.889 (0.889)  kappa 1.000 (1.000)  \n",
      "[ 7: 200]: eps 0.034255  Time 0.035 (0.037)  Total Loss 0.5636 (0.4549)  L1 Loss 0.0000 (0.0000)  CE 0.1791 (0.1476)  RCE 0.5636 (0.4549)  Err 0.0586 (0.0442)  Rob Err 0.1602 (0.1341)  Uns 19.0 (18.3)  Dead 41.3 (39.9)  Alive 131.7 (133.8)  Tightness 19.01953 (18.33567)  Bias -2.37593 (-2.53414)  Diff 69.40353 (71.99925)  R 29.936  beta 0.886 (0.886)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 7 eps:0.0350]: Time 0.017 (0.036)  Total Loss 0.4526 (0.4609)  L1 Loss 0.0000 (0.0000)  CE 0.1337 (0.1490)  RCE 0.4526 (0.4609)  Uns 18.896 (18.441)  Dead 41.2 (40.1)  Alive 131.9 (133.5)  Tight 18.89583 (18.44093)  Bias -2.24639 (-2.54446)  Diff 68.72860 (71.58645)  Err 0.0521 (0.0447)  Rob Err 0.1146 (0.1356)  R 32.873  beta 0.883 (0.883)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 28.663127899169922\n",
      "layer 3 norm 9.214004516601562\n",
      "layer 5 norm 7.547287464141846\n",
      "Epoch time: 8.7419, Total time: 62.8055\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 7 eps:0.0350]: Time 0.003 (0.017)  Total Loss 0.1942 (0.4664)  L1 Loss 0.0000 (0.0000)  CE 0.0237 (0.1391)  RCE 0.1942 (0.4664)  Uns 14.062 (18.713)  Dead 41.3 (41.7)  Alive 136.6 (131.6)  Tight 14.06250 (18.71350)  Bias -1.29395 (-2.58565)  Diff 69.74363 (68.44737)  Err 0.0000 (0.0405)  Rob Err 0.0625 (0.1410)  R 27.965  beta 0.883 (0.883)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 28.663127899169922\n",
      "layer 3 norm 9.214004516601562\n",
      "layer 5 norm 7.547287464141846\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 8, learning rate [0.0005], epsilon 0.035 - 0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8:   0]: eps 0.035000  Time 0.096 (0.096)  Total Loss 0.4410 (0.4410)  L1 Loss 0.0000 (0.0000)  CE 0.1189 (0.1189)  RCE 0.4410 (0.4410)  Err 0.0352 (0.0352)  Rob Err 0.1445 (0.1445)  Uns 18.9 (18.9)  Dead 41.3 (41.3)  Alive 131.8 (131.8)  Tightness 18.85156 (18.85156)  Bias -2.38988 (-2.38988)  Diff 69.64336 (69.64336)  R 29.933  beta 0.883 (0.883)  kappa 1.000 (1.000)  \n",
      "[ 8:  50]: eps 0.036064  Time 0.046 (0.040)  Total Loss 0.4402 (0.4639)  L1 Loss 0.0000 (0.0000)  CE 0.1288 (0.1384)  RCE 0.4402 (0.4639)  Err 0.0430 (0.0417)  Rob Err 0.1367 (0.1366)  Uns 19.8 (19.4)  Dead 41.8 (41.5)  Alive 130.4 (131.1)  Tightness 19.83594 (19.44248)  Bias -2.67944 (-2.69902)  Diff 67.39041 (68.42806)  R 33.158  beta 0.880 (0.880)  kappa 1.000 (1.000)  \n",
      "[ 8: 100]: eps 0.037128  Time 0.035 (0.037)  Total Loss 0.5224 (0.4817)  L1 Loss 0.0000 (0.0000)  CE 0.1843 (0.1437)  RCE 0.5224 (0.4817)  Err 0.0547 (0.0432)  Rob Err 0.1445 (0.1420)  Uns 19.6 (19.5)  Dead 41.4 (41.6)  Alive 131.0 (130.8)  Tightness 19.62500 (19.53241)  Bias -2.81391 (-2.71691)  Diff 66.80660 (67.87406)  R 31.578  beta 0.876 (0.876)  kappa 1.000 (1.000)  \n",
      "[ 8: 150]: eps 0.038191  Time 0.032 (0.036)  Total Loss 0.4178 (0.4924)  L1 Loss 0.0000 (0.0000)  CE 0.1194 (0.1457)  RCE 0.4178 (0.4924)  Err 0.0312 (0.0444)  Rob Err 0.1133 (0.1456)  Uns 20.2 (19.6)  Dead 43.2 (41.9)  Alive 128.6 (130.5)  Tightness 20.21484 (19.64236)  Bias -2.89613 (-2.75047)  Diff 64.55045 (67.09607)  R 29.366  beta 0.873 (0.873)  kappa 1.000 (1.000)  \n",
      "[ 8: 200]: eps 0.039255  Time 0.034 (0.036)  Total Loss 0.6647 (0.4944)  L1 Loss 0.0000 (0.0000)  CE 0.1943 (0.1453)  RCE 0.6647 (0.4944)  Err 0.0586 (0.0440)  Rob Err 0.1992 (0.1461)  Uns 20.1 (19.7)  Dead 42.3 (42.1)  Alive 129.7 (130.2)  Tightness 20.07031 (19.73492)  Bias -2.85585 (-2.76359)  Diff 64.33255 (66.35587)  R 32.266  beta 0.869 (0.869)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 8 eps:0.0400]: Time 0.016 (0.036)  Total Loss 0.5363 (0.5001)  L1 Loss 0.0000 (0.0000)  CE 0.1528 (0.1479)  RCE 0.5363 (0.5001)  Uns 20.000 (19.815)  Dead 43.9 (42.3)  Alive 128.1 (129.9)  Tight 20.00000 (19.81458)  Bias -2.64501 (-2.77735)  Diff 60.84851 (65.86658)  Err 0.0521 (0.0445)  Rob Err 0.1562 (0.1469)  R 29.485  beta 0.867 (0.867)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 26.065744400024414\n",
      "layer 3 norm 9.140728950500488\n",
      "layer 5 norm 7.505623817443848\n",
      "Epoch time: 8.5741, Total time: 71.3795\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 8 eps:0.0400]: Time 0.003 (0.017)  Total Loss 0.1560 (0.5086)  L1 Loss 0.0000 (0.0000)  CE 0.0163 (0.1431)  RCE 0.1560 (0.5086)  Uns 14.562 (19.877)  Dead 43.9 (43.6)  Alive 133.6 (128.6)  Tight 14.56250 (19.87700)  Bias -1.35057 (-2.71241)  Diff 62.39774 (61.80131)  Err 0.0000 (0.0421)  Rob Err 0.0625 (0.1511)  R 27.909  beta 0.867 (0.867)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 26.065744400024414\n",
      "layer 3 norm 9.140728950500488\n",
      "layer 5 norm 7.505623817443848\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 9, learning rate [0.0005], epsilon 0.04 - 0.045\n",
      "[ 9:   0]: eps 0.040000  Time 0.104 (0.104)  Total Loss 0.5884 (0.5884)  L1 Loss 0.0000 (0.0000)  CE 0.1846 (0.1846)  RCE 0.5884 (0.5884)  Err 0.0586 (0.0586)  Rob Err 0.1641 (0.1641)  Uns 20.0 (20.0)  Dead 43.1 (43.1)  Alive 128.9 (128.9)  Tightness 19.98438 (19.98438)  Bias -2.47208 (-2.47208)  Diff 62.24694 (62.24694)  R 29.022  beta 0.867 (0.867)  kappa 1.000 (1.000)  \n",
      "[ 9:  50]: eps 0.041064  Time 0.033 (0.039)  Total Loss 0.4949 (0.5185)  L1 Loss 0.0000 (0.0000)  CE 0.1177 (0.1411)  RCE 0.4949 (0.5185)  Err 0.0352 (0.0424)  Rob Err 0.1641 (0.1556)  Uns 20.1 (20.5)  Dead 44.1 (43.6)  Alive 127.8 (127.9)  Tightness 20.12109 (20.45964)  Bias -2.94685 (-2.83028)  Diff 61.01612 (61.56703)  R 30.014  beta 0.863 (0.863)  kappa 1.000 (1.000)  \n",
      "[ 9: 100]: eps 0.042128  Time 0.031 (0.037)  Total Loss 0.5037 (0.5250)  L1 Loss 0.0000 (0.0000)  CE 0.0928 (0.1444)  RCE 0.5037 (0.5250)  Err 0.0273 (0.0438)  Rob Err 0.1641 (0.1564)  Uns 20.3 (20.6)  Dead 44.3 (43.8)  Alive 127.4 (127.7)  Tightness 20.25391 (20.56772)  Bias -2.40151 (-2.87609)  Diff 59.17310 (60.91775)  R 32.614  beta 0.860 (0.860)  kappa 1.000 (1.000)  \n",
      "[ 9: 150]: eps 0.043191  Time 0.033 (0.036)  Total Loss 0.5290 (0.5329)  L1 Loss 0.0000 (0.0000)  CE 0.1409 (0.1478)  RCE 0.5290 (0.5329)  Err 0.0352 (0.0446)  Rob Err 0.1484 (0.1577)  Uns 21.6 (20.7)  Dead 44.6 (44.0)  Alive 125.8 (127.3)  Tightness 21.55859 (20.68776)  Bias -2.73326 (-2.89027)  Diff 58.44097 (60.09669)  R 29.366  beta 0.856 (0.856)  kappa 1.000 (1.000)  \n",
      "[ 9: 200]: eps 0.044255  Time 0.034 (0.035)  Total Loss 0.7048 (0.5391)  L1 Loss 0.0000 (0.0000)  CE 0.2305 (0.1495)  RCE 0.7048 (0.5391)  Err 0.0586 (0.0452)  Rob Err 0.1953 (0.1589)  Uns 22.0 (20.8)  Dead 44.8 (44.2)  Alive 125.2 (126.9)  Tightness 21.97656 (20.81705)  Bias -3.18427 (-2.90683)  Diff 56.09264 (59.31631)  R 27.222  beta 0.852 (0.852)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch: 9 eps:0.0450]: Time 0.016 (0.035)  Total Loss 0.5504 (0.5390)  L1 Loss 0.0000 (0.0000)  CE 0.1396 (0.1491)  RCE 0.5504 (0.5390)  Uns 20.927 (20.902)  Dead 44.8 (44.3)  Alive 126.3 (126.8)  Tight 20.92708 (20.90178)  Bias -4.00973 (-2.91291)  Diff 56.04293 (58.83197)  Err 0.0521 (0.0448)  Rob Err 0.1771 (0.1584)  R 29.589  beta 0.850 (0.850)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 23.60495376586914\n",
      "layer 3 norm 9.089951515197754\n",
      "layer 5 norm 7.456472396850586\n",
      "Epoch time: 8.4640, Total time: 79.8435\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch: 9 eps:0.0450]: Time 0.003 (0.017)  Total Loss 0.2576 (0.5511)  L1 Loss 0.0000 (0.0000)  CE 0.0330 (0.1481)  RCE 0.2576 (0.5511)  Uns 16.438 (21.340)  Dead 45.6 (45.3)  Alive 130.0 (125.4)  Tight 16.43750 (21.33950)  Bias -1.40430 (-2.93615)  Diff 55.14997 (55.26119)  Err 0.0000 (0.0433)  Rob Err 0.1250 (0.1660)  R 26.801  beta 0.850 (0.850)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 23.60495376586914\n",
      "layer 3 norm 9.089951515197754\n",
      "layer 5 norm 7.456472396850586\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 10, learning rate [0.0005], epsilon 0.045 - 0.05\n",
      "[10:   0]: eps 0.045000  Time 0.110 (0.110)  Total Loss 0.5664 (0.5664)  L1 Loss 0.0000 (0.0000)  CE 0.1393 (0.1393)  RCE 0.5664 (0.5664)  Err 0.0391 (0.0391)  Rob Err 0.1562 (0.1562)  Uns 21.4 (21.4)  Dead 45.2 (45.2)  Alive 125.3 (125.3)  Tightness 21.44141 (21.44141)  Bias -3.10159 (-3.10159)  Diff 54.82829 (54.82829)  R 27.725  beta 0.850 (0.850)  kappa 1.000 (1.000)  \n",
      "[10:  50]: eps 0.046064  Time 0.033 (0.039)  Total Loss 0.7170 (0.5660)  L1 Loss 0.0000 (0.0000)  CE 0.2757 (0.1543)  RCE 0.7170 (0.5660)  Err 0.0703 (0.0463)  Rob Err 0.1719 (0.1633)  Uns 21.5 (21.8)  Dead 45.6 (45.3)  Alive 124.9 (124.9)  Tightness 21.47266 (21.78048)  Bias -3.23733 (-2.99698)  Diff 54.10696 (54.91353)  R 27.947  beta 0.846 (0.846)  kappa 1.000 (1.000)  \n",
      "[10: 100]: eps 0.047128  Time 0.033 (0.037)  Total Loss 0.4631 (0.5662)  L1 Loss 0.0000 (0.0000)  CE 0.0929 (0.1508)  RCE 0.4631 (0.5662)  Err 0.0156 (0.0451)  Rob Err 0.1445 (0.1664)  Uns 21.5 (21.8)  Dead 46.3 (45.5)  Alive 124.2 (124.7)  Tightness 21.46875 (21.79162)  Bias -2.77709 (-2.95835)  Diff 51.96851 (53.97859)  R 28.577  beta 0.843 (0.843)  kappa 1.000 (1.000)  \n",
      "[10: 150]: eps 0.048191  Time 0.032 (0.036)  Total Loss 0.6128 (0.5729)  L1 Loss 0.0000 (0.0000)  CE 0.1538 (0.1526)  RCE 0.6128 (0.5729)  Err 0.0586 (0.0462)  Rob Err 0.1797 (0.1676)  Uns 21.5 (21.8)  Dead 46.4 (45.7)  Alive 124.2 (124.5)  Tightness 21.49609 (21.81739)  Bias -3.41384 (-2.96082)  Diff 50.86275 (53.27199)  R 29.180  beta 0.839 (0.839)  kappa 1.000 (1.000)  \n",
      "[10: 200]: eps 0.049255  Time 0.033 (0.036)  Total Loss 0.6169 (0.5744)  L1 Loss 0.0000 (0.0000)  CE 0.1642 (0.1519)  RCE 0.6169 (0.5744)  Err 0.0547 (0.0462)  Rob Err 0.1641 (0.1674)  Uns 22.5 (21.9)  Dead 46.4 (45.9)  Alive 123.1 (124.2)  Tightness 22.50781 (21.90699)  Bias -2.74786 (-2.95495)  Diff 49.98070 (52.48361)  R 27.830  beta 0.836 (0.836)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:10 eps:0.0500]: Time 0.016 (0.036)  Total Loss 0.5596 (0.5783)  L1 Loss 0.0000 (0.0000)  CE 0.1334 (0.1525)  RCE 0.5596 (0.5783)  Uns 22.417 (21.958)  Dead 48.2 (45.9)  Alive 121.4 (124.1)  Tight 22.41667 (21.95818)  Bias -2.96986 (-2.95364)  Diff 48.22739 (52.03308)  Err 0.0417 (0.0459)  Rob Err 0.1458 (0.1683)  R 25.937  beta 0.833 (0.833)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.17996597290039\n",
      "layer 3 norm 9.127237319946289\n",
      "layer 5 norm 7.451214790344238\n",
      "Epoch time: 8.5394, Total time: 88.3830\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL RESULT epoch:10 eps:0.0500]: Time 0.003 (0.016)  Total Loss 0.2339 (0.5716)  L1 Loss 0.0000 (0.0000)  CE 0.0266 (0.1440)  RCE 0.2339 (0.5716)  Uns 16.562 (21.686)  Dead 47.4 (47.0)  Alive 128.0 (123.3)  Tight 16.56250 (21.68580)  Bias -1.45928 (-2.74819)  Diff 48.10524 (48.30892)  Err 0.0000 (0.0435)  Rob Err 0.0625 (0.1713)  R 25.952  beta 0.833 (0.833)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.17996597290039\n",
      "layer 3 norm 9.127237319946289\n",
      "layer 5 norm 7.451214790344238\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 11, learning rate [0.0005], epsilon 0.05 - 0.055\n",
      "[11:   0]: eps 0.050000  Time 0.118 (0.118)  Total Loss 0.6646 (0.6646)  L1 Loss 0.0000 (0.0000)  CE 0.1943 (0.1943)  RCE 0.6646 (0.6646)  Err 0.0625 (0.0625)  Rob Err 0.1836 (0.1836)  Uns 22.3 (22.3)  Dead 47.0 (47.0)  Alive 122.7 (122.7)  Tightness 22.32031 (22.32031)  Bias -2.57277 (-2.57277)  Diff 47.87854 (47.87854)  R 29.759  beta 0.833 (0.833)  kappa 1.000 (1.000)  \n",
      "[11:  50]: eps 0.051064  Time 0.032 (0.040)  Total Loss 0.4561 (0.6034)  L1 Loss 0.0000 (0.0000)  CE 0.0911 (0.1555)  RCE 0.4561 (0.6034)  Err 0.0234 (0.0463)  Rob Err 0.1172 (0.1720)  Uns 21.7 (22.1)  Dead 47.2 (46.9)  Alive 123.2 (122.9)  Tightness 21.65625 (22.14024)  Bias -2.90151 (-2.86672)  Diff 47.62553 (47.88147)  R 28.670  beta 0.830 (0.830)  kappa 1.000 (1.000)  \n",
      "[11: 100]: eps 0.052128  Time 0.040 (0.037)  Total Loss 0.5418 (0.6014)  L1 Loss 0.0000 (0.0000)  CE 0.1331 (0.1525)  RCE 0.5418 (0.6014)  Err 0.0469 (0.0449)  Rob Err 0.1602 (0.1740)  Uns 22.6 (22.4)  Dead 47.4 (46.9)  Alive 121.9 (122.7)  Tightness 22.64453 (22.37674)  Bias -2.71779 (-2.85043)  Diff 47.20939 (47.39407)  R 28.826  beta 0.826 (0.826)  kappa 1.000 (1.000)  \n",
      "[11: 150]: eps 0.053191  Time 0.034 (0.036)  Total Loss 0.6424 (0.6060)  L1 Loss 0.0000 (0.0000)  CE 0.1634 (0.1534)  RCE 0.6424 (0.6060)  Err 0.0391 (0.0457)  Rob Err 0.1992 (0.1759)  Uns 22.6 (22.5)  Dead 47.6 (47.0)  Alive 121.8 (122.5)  Tightness 22.59375 (22.48272)  Bias -3.45000 (-2.83690)  Diff 45.05877 (46.80050)  R 26.201  beta 0.823 (0.823)  kappa 1.000 (1.000)  \n",
      "[11: 200]: eps 0.054255  Time 0.035 (0.036)  Total Loss 0.7248 (0.6129)  L1 Loss 0.0000 (0.0000)  CE 0.2449 (0.1553)  RCE 0.7248 (0.6129)  Err 0.0781 (0.0462)  Rob Err 0.2148 (0.1780)  Uns 22.4 (22.6)  Dead 48.1 (47.1)  Alive 121.5 (122.3)  Tightness 22.38281 (22.58269)  Bias -2.30523 (-2.82475)  Diff 43.13979 (46.14880)  R 28.771  beta 0.819 (0.819)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:11 eps:0.0550]: Time 0.019 (0.036)  Total Loss 0.5761 (0.6150)  L1 Loss 0.0000 (0.0000)  CE 0.1001 (0.1556)  RCE 0.5761 (0.6150)  Uns 24.615 (22.621)  Dead 47.2 (47.2)  Alive 120.2 (122.2)  Tight 24.61458 (22.62058)  Bias -3.26101 (-2.81717)  Diff 42.24349 (45.70609)  Err 0.0312 (0.0460)  Rob Err 0.1875 (0.1786)  R 25.961  beta 0.817 (0.817)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.04017448425293\n",
      "layer 3 norm 9.074146270751953\n",
      "layer 5 norm 7.455501079559326\n",
      "Epoch time: 8.5960, Total time: 96.9790\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:11 eps:0.0550]: Time 0.003 (0.016)  Total Loss 0.2502 (0.6093)  L1 Loss 0.0000 (0.0000)  CE 0.0329 (0.1479)  RCE 0.2502 (0.6093)  Uns 17.812 (22.558)  Dead 49.2 (48.1)  Alive 124.9 (121.4)  Tight 17.81250 (22.55840)  Bias -1.36534 (-2.64122)  Diff 41.76247 (42.36570)  Err 0.0000 (0.0421)  Rob Err 0.1250 (0.1808)  R 24.851  beta 0.817 (0.817)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.04017448425293\n",
      "layer 3 norm 9.074146270751953\n",
      "layer 5 norm 7.455501079559326\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 12, learning rate [0.0005], epsilon 0.055 - 0.06\n",
      "[12:   0]: eps 0.055000  Time 0.101 (0.101)  Total Loss 0.5752 (0.5752)  L1 Loss 0.0000 (0.0000)  CE 0.1263 (0.1263)  RCE 0.5752 (0.5752)  Err 0.0508 (0.0508)  Rob Err 0.1680 (0.1680)  Uns 23.3 (23.3)  Dead 47.9 (47.9)  Alive 120.7 (120.7)  Tightness 23.32812 (23.32812)  Bias -2.80149 (-2.80149)  Diff 42.45341 (42.45341)  R 25.918  beta 0.817 (0.817)  kappa 1.000 (1.000)  \n",
      "[12:  50]: eps 0.056064  Time 0.033 (0.040)  Total Loss 0.6788 (0.6307)  L1 Loss 0.0000 (0.0000)  CE 0.1771 (0.1525)  RCE 0.6788 (0.6307)  Err 0.0625 (0.0453)  Rob Err 0.1719 (0.1830)  Uns 23.6 (23.2)  Dead 47.3 (47.4)  Alive 121.1 (121.4)  Tightness 23.61719 (23.15686)  Bias -2.83624 (-2.74052)  Diff 42.16196 (42.45964)  R 24.403  beta 0.813 (0.813)  kappa 1.000 (1.000)  \n",
      "[12: 100]: eps 0.057128  Time 0.037 (0.038)  Total Loss 0.6524 (0.6390)  L1 Loss 0.0000 (0.0000)  CE 0.1509 (0.1575)  RCE 0.6524 (0.6390)  Err 0.0391 (0.0467)  Rob Err 0.1758 (0.1845)  Uns 23.2 (23.3)  Dead 48.3 (47.6)  Alive 120.5 (121.1)  Tightness 23.22656 (23.25955)  Bias -2.78893 (-2.69794)  Diff 39.94943 (41.76587)  R 25.195  beta 0.810 (0.810)  kappa 1.000 (1.000)  \n",
      "[12: 150]: eps 0.058191  Time 0.037 (0.037)  Total Loss 0.6387 (0.6467)  L1 Loss 0.0000 (0.0000)  CE 0.1526 (0.1595)  RCE 0.6387 (0.6467)  Err 0.0430 (0.0476)  Rob Err 0.1758 (0.1875)  Uns 23.6 (23.3)  Dead 48.3 (47.8)  Alive 120.1 (120.8)  Tightness 23.62891 (23.34610)  Bias -2.30974 (-2.67460)  Diff 38.64270 (41.06455)  R 26.437  beta 0.806 (0.806)  kappa 1.000 (1.000)  \n",
      "[12: 200]: eps 0.059255  Time 0.035 (0.037)  Total Loss 0.6299 (0.6532)  L1 Loss 0.0000 (0.0000)  CE 0.1418 (0.1610)  RCE 0.6299 (0.6532)  Err 0.0430 (0.0477)  Rob Err 0.1914 (0.1890)  Uns 23.1 (23.4)  Dead 48.9 (48.0)  Alive 120.1 (120.6)  Tightness 23.06641 (23.41702)  Bias -2.42043 (-2.62393)  Diff 38.33980 (40.42701)  R 26.112  beta 0.802 (0.802)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:12 eps:0.0600]: Time 0.016 (0.037)  Total Loss 0.9167 (0.6549)  L1 Loss 0.0000 (0.0000)  CE 0.2659 (0.1608)  RCE 0.9167 (0.6549)  Uns 23.938 (23.463)  Dead 47.4 (48.1)  Alive 120.7 (120.5)  Tight 23.93750 (23.46342)  Bias -2.70467 (-2.60389)  Diff 37.72776 (40.06468)  Err 0.0729 (0.0475)  Rob Err 0.2500 (0.1891)  R 25.256  beta 0.800 (0.800)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.843830108642578\n",
      "layer 3 norm 9.090313911437988\n",
      "layer 5 norm 7.4459452629089355\n",
      "Epoch time: 8.8084, Total time: 105.7874\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:12 eps:0.0600]: Time 0.003 (0.017)  Total Loss 0.2585 (0.6547)  L1 Loss 0.0000 (0.0000)  CE 0.0285 (0.1529)  RCE 0.2585 (0.6547)  Uns 19.062 (23.416)  Dead 49.5 (48.2)  Alive 123.4 (120.4)  Tight 19.06250 (23.41600)  Bias -1.52692 (-2.34489)  Diff 37.21176 (37.78639)  Err 0.0000 (0.0455)  Rob Err 0.1250 (0.1918)  R 25.791  beta 0.800 (0.800)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.843830108642578\n",
      "layer 3 norm 9.090313911437988\n",
      "layer 5 norm 7.4459452629089355\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 13, learning rate [0.0005], epsilon 0.06 - 0.065\n",
      "[13:   0]: eps 0.060000  Time 0.122 (0.122)  Total Loss 0.6993 (0.6993)  L1 Loss 0.0000 (0.0000)  CE 0.1535 (0.1535)  RCE 0.6993 (0.6993)  Err 0.0547 (0.0547)  Rob Err 0.2148 (0.2148)  Uns 23.5 (23.5)  Dead 48.0 (48.0)  Alive 120.5 (120.5)  Tightness 23.48047 (23.48047)  Bias -2.62620 (-2.62620)  Diff 38.34943 (38.34943)  R 25.374  beta 0.800 (0.800)  kappa 1.000 (1.000)  \n",
      "[13:  50]: eps 0.061064  Time 0.038 (0.041)  Total Loss 0.8365 (0.6751)  L1 Loss 0.0000 (0.0000)  CE 0.2463 (0.1613)  RCE 0.8365 (0.6751)  Err 0.0820 (0.0483)  Rob Err 0.2070 (0.1935)  Uns 24.4 (24.0)  Dead 48.9 (48.4)  Alive 118.7 (119.7)  Tightness 24.43750 (23.97763)  Bias -2.52430 (-2.43546)  Diff 36.29498 (37.22427)  R 24.955  beta 0.796 (0.796)  kappa 1.000 (1.000)  \n",
      "[13: 100]: eps 0.062128  Time 0.033 (0.038)  Total Loss 0.6798 (0.6858)  L1 Loss 0.0000 (0.0000)  CE 0.1592 (0.1660)  RCE 0.6798 (0.6858)  Err 0.0469 (0.0484)  Rob Err 0.2188 (0.1971)  Uns 23.5 (24.0)  Dead 49.1 (48.4)  Alive 119.4 (119.6)  Tightness 23.51953 (24.03636)  Bias -2.17924 (-2.36866)  Diff 35.04739 (36.60187)  R 25.327  beta 0.793 (0.793)  kappa 1.000 (1.000)  \n",
      "[13: 150]: eps 0.063191  Time 0.034 (0.037)  Total Loss 0.6985 (0.6911)  L1 Loss 0.0000 (0.0000)  CE 0.1370 (0.1660)  RCE 0.6985 (0.6911)  Err 0.0391 (0.0486)  Rob Err 0.2148 (0.1995)  Uns 24.4 (24.1)  Dead 49.0 (48.5)  Alive 118.7 (119.4)  Tightness 24.35156 (24.10288)  Bias -1.93224 (-2.35844)  Diff 33.53187 (36.02737)  R 24.039  beta 0.789 (0.789)  kappa 1.000 (1.000)  \n",
      "[13: 200]: eps 0.064255  Time 0.033 (0.036)  Total Loss 0.6908 (0.6929)  L1 Loss 0.0000 (0.0000)  CE 0.1564 (0.1658)  RCE 0.6908 (0.6929)  Err 0.0703 (0.0491)  Rob Err 0.2188 (0.1995)  Uns 23.7 (24.2)  Dead 49.8 (48.5)  Alive 118.5 (119.3)  Tightness 23.66016 (24.18587)  Bias -2.12460 (-2.33135)  Diff 33.01923 (35.52168)  R 25.530  beta 0.786 (0.786)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL RESULT epoch:13 eps:0.0650]: Time 0.017 (0.036)  Total Loss 0.8064 (0.6971)  L1 Loss 0.0000 (0.0000)  CE 0.1593 (0.1670)  RCE 0.8064 (0.6971)  Uns 24.573 (24.201)  Dead 49.6 (48.6)  Alive 117.8 (119.2)  Tight 24.57292 (24.20050)  Bias -2.22339 (-2.30441)  Diff 32.56457 (35.14496)  Err 0.0312 (0.0490)  Rob Err 0.2188 (0.2006)  R 23.366  beta 0.783 (0.783)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.391611099243164\n",
      "layer 3 norm 9.074681282043457\n",
      "layer 5 norm 7.449430465698242\n",
      "Epoch time: 8.6502, Total time: 114.4377\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:13 eps:0.0650]: Time 0.003 (0.017)  Total Loss 0.2758 (0.6885)  L1 Loss 0.0000 (0.0000)  CE 0.0340 (0.1592)  RCE 0.2758 (0.6885)  Uns 20.062 (23.981)  Dead 51.2 (49.5)  Alive 120.7 (118.5)  Tight 20.06250 (23.98080)  Bias -1.02431 (-1.97688)  Diff 31.70740 (32.38111)  Err 0.0000 (0.0452)  Rob Err 0.1875 (0.2026)  R 24.026  beta 0.783 (0.783)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.391611099243164\n",
      "layer 3 norm 9.074681282043457\n",
      "layer 5 norm 7.449430465698242\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 14, learning rate [0.0005], epsilon 0.065 - 0.07\n",
      "[14:   0]: eps 0.065000  Time 0.100 (0.100)  Total Loss 0.6649 (0.6649)  L1 Loss 0.0000 (0.0000)  CE 0.1347 (0.1347)  RCE 0.6649 (0.6649)  Err 0.0391 (0.0391)  Rob Err 0.1953 (0.1953)  Uns 23.9 (23.9)  Dead 49.5 (49.5)  Alive 118.6 (118.6)  Tightness 23.91406 (23.91406)  Bias -1.90896 (-1.90896)  Diff 32.73331 (32.73331)  R 24.933  beta 0.783 (0.783)  kappa 1.000 (1.000)  \n",
      "[14:  50]: eps 0.066064  Time 0.032 (0.039)  Total Loss 0.6028 (0.7209)  L1 Loss 0.0000 (0.0000)  CE 0.1140 (0.1676)  RCE 0.6028 (0.7209)  Err 0.0312 (0.0495)  Rob Err 0.1875 (0.2080)  Uns 25.1 (24.6)  Dead 49.0 (49.0)  Alive 118.0 (118.5)  Tightness 25.07422 (24.56296)  Bias -2.17238 (-2.07095)  Diff 32.42345 (32.50450)  R 24.800  beta 0.780 (0.780)  kappa 1.000 (1.000)  \n",
      "[14: 100]: eps 0.067128  Time 0.032 (0.037)  Total Loss 0.7099 (0.7216)  L1 Loss 0.0000 (0.0000)  CE 0.1844 (0.1712)  RCE 0.7099 (0.7216)  Err 0.0586 (0.0493)  Rob Err 0.1641 (0.2047)  Uns 24.5 (24.7)  Dead 49.1 (49.0)  Alive 118.4 (118.3)  Tightness 24.46484 (24.69995)  Bias -1.48094 (-2.06337)  Diff 31.00270 (32.05738)  R 24.216  beta 0.776 (0.776)  kappa 1.000 (1.000)  \n",
      "[14: 150]: eps 0.068191  Time 0.031 (0.036)  Total Loss 0.6468 (0.7318)  L1 Loss 0.0000 (0.0000)  CE 0.1129 (0.1734)  RCE 0.6468 (0.7318)  Err 0.0352 (0.0504)  Rob Err 0.1914 (0.2083)  Uns 24.1 (24.7)  Dead 49.8 (49.1)  Alive 118.1 (118.2)  Tightness 24.12891 (24.71691)  Bias -2.15376 (-2.02397)  Diff 29.44638 (31.54147)  R 27.001  beta 0.773 (0.773)  kappa 1.000 (1.000)  \n",
      "[14: 200]: eps 0.069255  Time 0.035 (0.036)  Total Loss 0.6341 (0.7380)  L1 Loss 0.0000 (0.0000)  CE 0.1219 (0.1741)  RCE 0.6341 (0.7380)  Err 0.0430 (0.0508)  Rob Err 0.1836 (0.2107)  Uns 24.9 (24.8)  Dead 49.0 (49.1)  Alive 118.1 (118.1)  Tightness 24.92969 (24.77408)  Bias -2.58973 (-2.00223)  Diff 29.55284 (31.17085)  R 26.188  beta 0.769 (0.769)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:14 eps:0.0700]: Time 0.017 (0.036)  Total Loss 0.9269 (0.7420)  L1 Loss 0.0000 (0.0000)  CE 0.1713 (0.1739)  RCE 0.9269 (0.7420)  Uns 24.698 (24.836)  Dead 48.4 (49.1)  Alive 118.9 (118.0)  Tight 24.69792 (24.83578)  Bias -2.61675 (-1.97542)  Diff 29.62360 (30.88481)  Err 0.0417 (0.0506)  Rob Err 0.2812 (0.2128)  R 22.515  beta 0.767 (0.767)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.885210037231445\n",
      "layer 3 norm 9.032657623291016\n",
      "layer 5 norm 7.464093208312988\n",
      "Epoch time: 8.6650, Total time: 123.1027\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:14 eps:0.0700]: Time 0.003 (0.017)  Total Loss 0.2548 (0.7301)  L1 Loss 0.0000 (0.0000)  CE 0.0281 (0.1648)  RCE 0.2548 (0.7301)  Uns 20.812 (24.465)  Dead 51.2 (49.6)  Alive 119.9 (117.9)  Tight 20.81250 (24.46470)  Bias -0.78650 (-1.61503)  Diff 27.81849 (28.62806)  Err 0.0000 (0.0479)  Rob Err 0.0625 (0.2150)  R 22.941  beta 0.767 (0.767)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.885210037231445\n",
      "layer 3 norm 9.032657623291016\n",
      "layer 5 norm 7.464093208312988\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 15, learning rate [0.0005], epsilon 0.07 - 0.075\n",
      "[15:   0]: eps 0.070000  Time 0.103 (0.103)  Total Loss 0.8176 (0.8176)  L1 Loss 0.0000 (0.0000)  CE 0.1780 (0.1780)  RCE 0.8176 (0.8176)  Err 0.0469 (0.0469)  Rob Err 0.2539 (0.2539)  Uns 24.0 (24.0)  Dead 49.1 (49.1)  Alive 118.9 (118.9)  Tightness 23.98438 (23.98438)  Bias -1.54283 (-1.54283)  Diff 28.67561 (28.67561)  R 24.291  beta 0.767 (0.767)  kappa 1.000 (1.000)  \n",
      "[15:  50]: eps 0.071064  Time 0.034 (0.040)  Total Loss 0.6568 (0.7744)  L1 Loss 0.0000 (0.0000)  CE 0.1392 (0.1838)  RCE 0.6568 (0.7744)  Err 0.0352 (0.0535)  Rob Err 0.1875 (0.2222)  Uns 25.5 (25.2)  Dead 49.4 (49.5)  Alive 117.0 (117.3)  Tightness 25.53125 (25.15326)  Bias -2.40825 (-1.63959)  Diff 28.32361 (28.27679)  R 25.315  beta 0.763 (0.763)  kappa 1.000 (1.000)  \n",
      "[15: 100]: eps 0.072128  Time 0.035 (0.039)  Total Loss 0.9784 (0.7763)  L1 Loss 0.0000 (0.0000)  CE 0.2464 (0.1832)  RCE 0.9784 (0.7763)  Err 0.0742 (0.0532)  Rob Err 0.2891 (0.2216)  Uns 25.7 (25.2)  Dead 49.4 (49.6)  Alive 116.8 (117.2)  Tightness 25.71875 (25.20135)  Bias -1.65021 (-1.61385)  Diff 27.71054 (27.81989)  R 23.993  beta 0.760 (0.760)  kappa 1.000 (1.000)  \n",
      "[15: 150]: eps 0.073191  Time 0.036 (0.038)  Total Loss 0.7522 (0.7778)  L1 Loss 0.0000 (0.0000)  CE 0.1734 (0.1814)  RCE 0.7522 (0.7778)  Err 0.0586 (0.0525)  Rob Err 0.2188 (0.2227)  Uns 25.4 (25.3)  Dead 49.3 (49.6)  Alive 117.3 (117.1)  Tightness 25.37109 (25.27716)  Bias -1.33768 (-1.57934)  Diff 26.45657 (27.47972)  R 24.891  beta 0.756 (0.756)  kappa 1.000 (1.000)  \n",
      "[15: 200]: eps 0.074255  Time 0.039 (0.038)  Total Loss 0.8124 (0.7822)  L1 Loss 0.0000 (0.0000)  CE 0.2111 (0.1822)  RCE 0.8124 (0.7822)  Err 0.0586 (0.0527)  Rob Err 0.2109 (0.2241)  Uns 25.4 (25.3)  Dead 50.0 (49.6)  Alive 116.6 (117.0)  Tightness 25.38281 (25.34715)  Bias -1.39255 (-1.55739)  Diff 25.77889 (27.18263)  R 25.148  beta 0.752 (0.752)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:15 eps:0.0750]: Time 0.022 (0.038)  Total Loss 0.7939 (0.7869)  L1 Loss 0.0000 (0.0000)  CE 0.1867 (0.1822)  RCE 0.7939 (0.7869)  Uns 24.865 (25.402)  Dead 49.8 (49.6)  Alive 117.3 (117.0)  Tight 24.86458 (25.40173)  Bias -1.02682 (-1.53263)  Diff 25.16844 (26.95151)  Err 0.0625 (0.0528)  Rob Err 0.2500 (0.2259)  R 23.159  beta 0.750 (0.750)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.618160247802734\n",
      "layer 3 norm 8.98188591003418\n",
      "layer 5 norm 7.445223808288574\n",
      "Epoch time: 9.1394, Total time: 132.2421\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:15 eps:0.0750]: Time 0.003 (0.018)  Total Loss 0.2699 (0.7748)  L1 Loss 0.0000 (0.0000)  CE 0.0302 (0.1729)  RCE 0.2699 (0.7748)  Uns 22.000 (25.169)  Dead 51.5 (49.9)  Alive 118.5 (116.9)  Tight 22.00000 (25.16920)  Bias -0.23039 (-1.26906)  Diff 24.16911 (25.24469)  Err 0.0000 (0.0487)  Rob Err 0.1250 (0.2259)  R 22.857  beta 0.750 (0.750)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.618160247802734\n",
      "layer 3 norm 8.98188591003418\n",
      "layer 5 norm 7.445223808288574\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 16, learning rate [0.0005], epsilon 0.075 - 0.08\n",
      "[16:   0]: eps 0.075000  Time 0.111 (0.111)  Total Loss 0.7385 (0.7385)  L1 Loss 0.0000 (0.0000)  CE 0.1747 (0.1747)  RCE 0.7385 (0.7385)  Err 0.0508 (0.0508)  Rob Err 0.2070 (0.2070)  Uns 25.9 (25.9)  Dead 49.4 (49.4)  Alive 116.7 (116.7)  Tightness 25.88672 (25.88672)  Bias -1.46445 (-1.46445)  Diff 25.09836 (25.09836)  R 25.405  beta 0.750 (0.750)  kappa 1.000 (1.000)  \n",
      "[16:  50]: eps 0.076064  Time 0.036 (0.043)  Total Loss 0.6708 (0.8164)  L1 Loss 0.0000 (0.0000)  CE 0.1113 (0.1901)  RCE 0.6708 (0.8164)  Err 0.0312 (0.0555)  Rob Err 0.1992 (0.2316)  Uns 25.9 (25.8)  Dead 49.5 (49.6)  Alive 116.6 (116.6)  Tightness 25.87891 (25.78301)  Bias -1.29860 (-1.32856)  Diff 24.80645 (25.01273)  R 25.962  beta 0.746 (0.746)  kappa 1.000 (1.000)  \n",
      "[16: 100]: eps 0.077128  Time 0.053 (0.040)  Total Loss 0.8328 (0.8203)  L1 Loss 0.0000 (0.0000)  CE 0.1713 (0.1880)  RCE 0.8328 (0.8203)  Err 0.0625 (0.0543)  Rob Err 0.2812 (0.2348)  Uns 24.6 (25.7)  Dead 50.4 (49.8)  Alive 117.0 (116.5)  Tightness 24.58594 (25.69009)  Bias -0.22369 (-1.29503)  Diff 23.61989 (24.57614)  R 23.794  beta 0.743 (0.743)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16: 150]: eps 0.078191  Time 0.041 (0.039)  Total Loss 0.7256 (0.8261)  L1 Loss 0.0000 (0.0000)  CE 0.1372 (0.1874)  RCE 0.7256 (0.8261)  Err 0.0273 (0.0548)  Rob Err 0.1953 (0.2373)  Uns 25.8 (25.8)  Dead 50.3 (49.9)  Alive 115.8 (116.4)  Tightness 25.84766 (25.76485)  Bias -1.41456 (-1.27407)  Diff 23.14619 (24.28529)  R 24.055  beta 0.739 (0.739)  kappa 1.000 (1.000)  \n",
      "[16: 200]: eps 0.079255  Time 0.044 (0.039)  Total Loss 0.8175 (0.8327)  L1 Loss 0.0000 (0.0000)  CE 0.1783 (0.1904)  RCE 0.8175 (0.8327)  Err 0.0508 (0.0552)  Rob Err 0.2578 (0.2389)  Uns 26.2 (25.8)  Dead 49.8 (49.9)  Alive 116.0 (116.3)  Tightness 26.20312 (25.79818)  Bias -0.34172 (-1.19878)  Diff 22.09550 (23.93915)  R 24.961  beta 0.736 (0.736)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:16 eps:0.0800]: Time 0.019 (0.039)  Total Loss 0.8469 (0.8344)  L1 Loss 0.0000 (0.0000)  CE 0.2719 (0.1916)  RCE 0.8469 (0.8344)  Uns 27.115 (25.817)  Dead 49.6 (49.9)  Alive 115.2 (116.3)  Tight 27.11458 (25.81687)  Bias -1.05703 (-1.17669)  Diff 22.53532 (23.72105)  Err 0.0417 (0.0552)  Rob Err 0.1771 (0.2391)  R 24.959  beta 0.733 (0.733)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.043025970458984\n",
      "layer 3 norm 8.88118839263916\n",
      "layer 5 norm 7.425075054168701\n",
      "Epoch time: 9.3645, Total time: 141.6066\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:16 eps:0.0800]: Time 0.004 (0.019)  Total Loss 0.2788 (0.8216)  L1 Loss 0.0000 (0.0000)  CE 0.0328 (0.1837)  RCE 0.2788 (0.8216)  Uns 22.938 (25.684)  Dead 51.9 (50.2)  Alive 117.1 (116.1)  Tight 22.93750 (25.68420)  Bias 0.11588 (-0.85920)  Diff 20.97165 (22.22533)  Err 0.0000 (0.0507)  Rob Err 0.0625 (0.2386)  R 22.109  beta 0.733 (0.733)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.043025970458984\n",
      "layer 3 norm 8.88118839263916\n",
      "layer 5 norm 7.425075054168701\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 17, learning rate [0.0005], epsilon 0.08 - 0.085\n",
      "[17:   0]: eps 0.080000  Time 0.118 (0.118)  Total Loss 0.9101 (0.9101)  L1 Loss 0.0000 (0.0000)  CE 0.1991 (0.1991)  RCE 0.9101 (0.9101)  Err 0.0664 (0.0664)  Rob Err 0.2422 (0.2422)  Uns 26.2 (26.2)  Dead 49.1 (49.1)  Alive 116.6 (116.6)  Tightness 26.23047 (26.23047)  Bias -0.79802 (-0.79802)  Diff 22.67633 (22.67633)  R 24.619  beta 0.733 (0.733)  kappa 1.000 (1.000)  \n",
      "[17:  50]: eps 0.081064  Time 0.033 (0.043)  Total Loss 0.9881 (0.8923)  L1 Loss 0.0000 (0.0000)  CE 0.2414 (0.2101)  RCE 0.9881 (0.8923)  Err 0.0703 (0.0620)  Rob Err 0.2656 (0.2518)  Uns 25.9 (26.0)  Dead 50.3 (50.0)  Alive 115.9 (115.9)  Tightness 25.85938 (26.03615)  Bias -1.06552 (-1.05511)  Diff 21.29282 (22.07112)  R 23.818  beta 0.730 (0.730)  kappa 1.000 (1.000)  \n",
      "[17: 100]: eps 0.082128  Time 0.037 (0.039)  Total Loss 0.8383 (0.8714)  L1 Loss 0.0000 (0.0000)  CE 0.1740 (0.2011)  RCE 0.8383 (0.8714)  Err 0.0508 (0.0584)  Rob Err 0.2266 (0.2476)  Uns 26.1 (26.0)  Dead 49.8 (50.0)  Alive 116.1 (116.0)  Tightness 26.13281 (26.02715)  Bias -1.43537 (-0.97428)  Diff 21.74936 (21.74464)  R 23.991  beta 0.726 (0.726)  kappa 1.000 (1.000)  \n",
      "[17: 150]: eps 0.083191  Time 0.036 (0.039)  Total Loss 0.8715 (0.8764)  L1 Loss 0.0000 (0.0000)  CE 0.2178 (0.2021)  RCE 0.8715 (0.8764)  Err 0.0547 (0.0582)  Rob Err 0.2500 (0.2499)  Uns 26.4 (26.0)  Dead 49.6 (50.0)  Alive 116.0 (115.9)  Tightness 26.37500 (26.03792)  Bias -0.94314 (-0.94591)  Diff 20.49712 (21.36671)  R 25.204  beta 0.723 (0.723)  kappa 1.000 (1.000)  \n",
      "[17: 200]: eps 0.084255  Time 0.034 (0.038)  Total Loss 0.9156 (0.8765)  L1 Loss 0.0000 (0.0000)  CE 0.1998 (0.2004)  RCE 0.9156 (0.8765)  Err 0.0625 (0.0576)  Rob Err 0.2617 (0.2505)  Uns 26.3 (26.1)  Dead 50.6 (50.1)  Alive 115.1 (115.9)  Tightness 26.32031 (26.05486)  Bias -1.27702 (-0.90694)  Diff 20.12511 (21.06579)  R 24.580  beta 0.719 (0.719)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:17 eps:0.0850]: Time 0.018 (0.037)  Total Loss 0.9616 (0.8808)  L1 Loss 0.0000 (0.0000)  CE 0.2255 (0.2006)  RCE 0.9616 (0.8808)  Uns 25.583 (26.089)  Dead 50.1 (50.0)  Alive 116.3 (115.9)  Tight 25.58333 (26.08892)  Bias -1.04310 (-0.89526)  Diff 19.56584 (20.90362)  Err 0.0625 (0.0573)  Rob Err 0.2812 (0.2518)  R 23.294  beta 0.717 (0.717)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.1702938079834\n",
      "layer 3 norm 8.730097770690918\n",
      "layer 5 norm 7.414680004119873\n",
      "Epoch time: 8.9883, Total time: 150.5949\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:17 eps:0.0850]: Time 0.003 (0.017)  Total Loss 0.3241 (0.8596)  L1 Loss 0.0000 (0.0000)  CE 0.0342 (0.1914)  RCE 0.3241 (0.8596)  Uns 22.562 (25.341)  Dead 52.6 (50.8)  Alive 116.9 (115.8)  Tight 22.56250 (25.34100)  Bias 0.45052 (-0.55822)  Diff 17.44132 (19.17483)  Err 0.0000 (0.0541)  Rob Err 0.1875 (0.2504)  R 22.266  beta 0.717 (0.717)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.1702938079834\n",
      "layer 3 norm 8.730097770690918\n",
      "layer 5 norm 7.414680004119873\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 18, learning rate [0.0005], epsilon 0.085 - 0.09\n",
      "[18:   0]: eps 0.085000  Time 0.106 (0.106)  Total Loss 0.9312 (0.9312)  L1 Loss 0.0000 (0.0000)  CE 0.2296 (0.2296)  RCE 0.9312 (0.9312)  Err 0.0742 (0.0742)  Rob Err 0.2227 (0.2227)  Uns 26.6 (26.6)  Dead 50.2 (50.2)  Alive 115.2 (115.2)  Tightness 26.62500 (26.62500)  Bias -0.87002 (-0.87002)  Diff 19.81569 (19.81569)  R 24.072  beta 0.717 (0.717)  kappa 1.000 (1.000)  \n",
      "[18:  50]: eps 0.086064  Time 0.040 (0.041)  Total Loss 0.8660 (0.8941)  L1 Loss 0.0000 (0.0000)  CE 0.2105 (0.2023)  RCE 0.8660 (0.8941)  Err 0.0625 (0.0593)  Rob Err 0.2578 (0.2528)  Uns 26.5 (26.2)  Dead 50.0 (50.2)  Alive 115.6 (115.6)  Tightness 26.46484 (26.19424)  Bias -1.71778 (-0.78533)  Diff 19.49152 (19.49685)  R 25.268  beta 0.713 (0.713)  kappa 1.000 (1.000)  \n",
      "[18: 100]: eps 0.087128  Time 0.031 (0.038)  Total Loss 0.9733 (0.9091)  L1 Loss 0.0000 (0.0000)  CE 0.2589 (0.2063)  RCE 0.9733 (0.9091)  Err 0.0820 (0.0591)  Rob Err 0.2539 (0.2577)  Uns 24.8 (26.1)  Dead 51.5 (50.3)  Alive 115.7 (115.6)  Tightness 24.80469 (26.14016)  Bias -1.72247 (-0.71731)  Diff 18.55169 (19.25613)  R 24.400  beta 0.710 (0.710)  kappa 1.000 (1.000)  \n",
      "[18: 150]: eps 0.088191  Time 0.031 (0.037)  Total Loss 1.0016 (0.9185)  L1 Loss 0.0000 (0.0000)  CE 0.2301 (0.2088)  RCE 1.0016 (0.9185)  Err 0.0625 (0.0590)  Rob Err 0.2656 (0.2597)  Uns 25.7 (26.1)  Dead 50.4 (50.2)  Alive 115.9 (115.7)  Tightness 25.72656 (26.08418)  Bias -0.59984 (-0.70930)  Diff 18.65128 (18.94414)  R 24.649  beta 0.706 (0.706)  kappa 1.000 (1.000)  \n",
      "[18: 200]: eps 0.089255  Time 0.035 (0.036)  Total Loss 0.8522 (0.9185)  L1 Loss 0.0000 (0.0000)  CE 0.1818 (0.2084)  RCE 0.8522 (0.9185)  Err 0.0547 (0.0587)  Rob Err 0.2500 (0.2612)  Uns 26.2 (26.1)  Dead 50.2 (50.2)  Alive 115.5 (115.7)  Tightness 26.23438 (26.05960)  Bias -1.20715 (-0.66468)  Diff 17.57576 (18.65045)  R 25.985  beta 0.702 (0.702)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:18 eps:0.0900]: Time 0.017 (0.036)  Total Loss 0.9924 (0.9282)  L1 Loss 0.0000 (0.0000)  CE 0.1687 (0.2111)  RCE 0.9924 (0.9282)  Uns 26.073 (26.032)  Dead 49.8 (50.2)  Alive 116.1 (115.8)  Tight 26.07292 (26.03210)  Bias 1.11611 (-0.68710)  Diff 16.15084 (18.45718)  Err 0.0417 (0.0590)  Rob Err 0.2812 (0.2647)  R 24.127  beta 0.700 (0.700)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.288183212280273\n",
      "layer 3 norm 8.612348556518555\n",
      "layer 5 norm 7.417516231536865\n",
      "Epoch time: 8.6240, Total time: 159.2188\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:18 eps:0.0900]: Time 0.003 (0.019)  Total Loss 0.3535 (0.9070)  L1 Loss 0.0000 (0.0000)  CE 0.0395 (0.2052)  RCE 0.3535 (0.9070)  Uns 23.000 (25.536)  Dead 52.8 (51.0)  Alive 116.2 (115.5)  Tight 23.00000 (25.53610)  Bias 0.84141 (-0.46175)  Diff 15.31200 (16.91363)  Err 0.0000 (0.0551)  Rob Err 0.1250 (0.2636)  R 22.340  beta 0.700 (0.700)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.288183212280273\n",
      "layer 3 norm 8.612348556518555\n",
      "layer 5 norm 7.417516231536865\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 19, learning rate [0.0005], epsilon 0.09 - 0.095\n",
      "[19:   0]: eps 0.090000  Time 0.119 (0.119)  Total Loss 0.9306 (0.9306)  L1 Loss 0.0000 (0.0000)  CE 0.2357 (0.2357)  RCE 0.9306 (0.9306)  Err 0.0625 (0.0625)  Rob Err 0.2383 (0.2383)  Uns 25.5 (25.5)  Dead 50.8 (50.8)  Alive 115.7 (115.7)  Tightness 25.50781 (25.50781)  Bias -1.20841 (-1.20841)  Diff 17.16593 (17.16593)  R 24.275  beta 0.700 (0.700)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:  50]: eps 0.091064  Time 0.033 (0.040)  Total Loss 0.9580 (0.9621)  L1 Loss 0.0000 (0.0000)  CE 0.2263 (0.2236)  RCE 0.9580 (0.9621)  Err 0.0625 (0.0606)  Rob Err 0.2500 (0.2747)  Uns 25.9 (26.0)  Dead 50.1 (50.3)  Alive 115.9 (115.6)  Tightness 25.94922 (26.03378)  Bias -0.85618 (-0.69835)  Diff 16.53714 (17.05492)  R 24.386  beta 0.696 (0.696)  kappa 1.000 (1.000)  \n",
      "[19: 100]: eps 0.092128  Time 0.033 (0.037)  Total Loss 0.8395 (0.9626)  L1 Loss 0.0000 (0.0000)  CE 0.1712 (0.2221)  RCE 0.8395 (0.9626)  Err 0.0469 (0.0611)  Rob Err 0.2578 (0.2740)  Uns 26.1 (26.0)  Dead 49.8 (50.3)  Alive 116.1 (115.7)  Tightness 26.14844 (25.96867)  Bias 0.90161 (-0.67986)  Diff 15.59745 (16.77023)  R 24.598  beta 0.693 (0.693)  kappa 1.000 (1.000)  \n",
      "[19: 150]: eps 0.093191  Time 0.034 (0.036)  Total Loss 0.9044 (0.9698)  L1 Loss 0.0000 (0.0000)  CE 0.1861 (0.2217)  RCE 0.9044 (0.9698)  Err 0.0508 (0.0615)  Rob Err 0.2617 (0.2766)  Uns 25.9 (26.0)  Dead 50.6 (50.3)  Alive 115.5 (115.8)  Tightness 25.89453 (25.97279)  Bias -0.45678 (-0.67066)  Diff 16.13401 (16.62264)  R 24.831  beta 0.689 (0.689)  kappa 1.000 (1.000)  \n",
      "[19: 200]: eps 0.094255  Time 0.034 (0.036)  Total Loss 0.9826 (0.9728)  L1 Loss 0.0000 (0.0000)  CE 0.1784 (0.2234)  RCE 0.9826 (0.9728)  Err 0.0508 (0.0621)  Rob Err 0.2969 (0.2771)  Uns 25.5 (26.0)  Dead 50.8 (50.2)  Alive 115.8 (115.8)  Tightness 25.48047 (25.96811)  Bias -0.21435 (-0.64338)  Diff 15.02672 (16.38715)  R 25.150  beta 0.686 (0.686)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:19 eps:0.0950]: Time 0.018 (0.035)  Total Loss 0.9927 (0.9753)  L1 Loss 0.0000 (0.0000)  CE 0.1780 (0.2235)  RCE 0.9927 (0.9753)  Uns 26.927 (25.955)  Dead 48.2 (50.3)  Alive 116.8 (115.8)  Tight 26.92708 (25.95530)  Bias -0.81310 (-0.62635)  Diff 14.95599 (16.25397)  Err 0.0312 (0.0620)  Rob Err 0.3021 (0.2778)  R 23.240  beta 0.683 (0.683)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.040761947631836\n",
      "layer 3 norm 8.946670532226562\n",
      "layer 5 norm 7.466217517852783\n",
      "Epoch time: 8.5059, Total time: 167.7247\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:19 eps:0.0950]: Time 0.003 (0.017)  Total Loss 0.4133 (0.9518)  L1 Loss 0.0000 (0.0000)  CE 0.0461 (0.2116)  RCE 0.4133 (0.9518)  Uns 23.250 (26.056)  Dead 52.1 (50.0)  Alive 116.6 (115.9)  Tight 23.25000 (26.05640)  Bias 1.04805 (-0.42619)  Diff 13.67521 (15.15985)  Err 0.0000 (0.0560)  Rob Err 0.2500 (0.2758)  R 22.260  beta 0.683 (0.683)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 21.040761947631836\n",
      "layer 3 norm 8.946670532226562\n",
      "layer 5 norm 7.466217517852783\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 20, learning rate [0.0005], epsilon 0.095 - 0.1\n",
      "[20:   0]: eps 0.095000  Time 0.121 (0.121)  Total Loss 1.0474 (1.0474)  L1 Loss 0.0000 (0.0000)  CE 0.2255 (0.2255)  RCE 1.0474 (1.0474)  Err 0.0586 (0.0586)  Rob Err 0.3281 (0.3281)  Uns 26.5 (26.5)  Dead 49.5 (49.5)  Alive 115.9 (115.9)  Tightness 26.54297 (26.54297)  Bias -1.84438 (-1.84438)  Diff 15.56996 (15.56996)  R 24.538  beta 0.683 (0.683)  kappa 1.000 (1.000)  \n",
      "[20:  50]: eps 0.096064  Time 0.039 (0.041)  Total Loss 0.9686 (1.0049)  L1 Loss 0.0000 (0.0000)  CE 0.1869 (0.2304)  RCE 0.9686 (1.0049)  Err 0.0547 (0.0636)  Rob Err 0.3320 (0.2891)  Uns 26.0 (26.0)  Dead 50.1 (50.1)  Alive 115.9 (115.9)  Tightness 25.96875 (25.95312)  Bias -0.65425 (-0.70105)  Diff 14.85891 (15.26192)  R 24.417  beta 0.680 (0.680)  kappa 1.000 (1.000)  \n",
      "[20: 100]: eps 0.097128  Time 0.033 (0.038)  Total Loss 0.9869 (1.0165)  L1 Loss 0.0000 (0.0000)  CE 0.1936 (0.2354)  RCE 0.9869 (1.0165)  Err 0.0508 (0.0652)  Rob Err 0.2812 (0.2917)  Uns 26.0 (25.9)  Dead 49.7 (50.2)  Alive 116.3 (115.9)  Tightness 25.99609 (25.93344)  Bias -0.98928 (-0.70537)  Diff 14.73142 (14.98684)  R 26.054  beta 0.676 (0.676)  kappa 1.000 (1.000)  \n",
      "[20: 150]: eps 0.098191  Time 0.035 (0.038)  Total Loss 0.9989 (1.0139)  L1 Loss 0.0000 (0.0000)  CE 0.1938 (0.2322)  RCE 0.9989 (1.0139)  Err 0.0508 (0.0644)  Rob Err 0.2969 (0.2901)  Uns 26.0 (25.9)  Dead 50.0 (50.1)  Alive 116.0 (115.9)  Tightness 26.00391 (25.94433)  Bias -0.51586 (-0.72688)  Diff 14.03158 (14.74806)  R 25.002  beta 0.673 (0.673)  kappa 1.000 (1.000)  \n",
      "[20: 200]: eps 0.099255  Time 0.034 (0.038)  Total Loss 1.0055 (1.0167)  L1 Loss 0.0000 (0.0000)  CE 0.2108 (0.2334)  RCE 1.0055 (1.0167)  Err 0.0664 (0.0638)  Rob Err 0.2969 (0.2904)  Uns 26.0 (26.0)  Dead 50.0 (50.1)  Alive 116.0 (115.9)  Tightness 26.04297 (25.95623)  Bias -0.52271 (-0.69089)  Diff 13.17246 (14.54869)  R 25.214  beta 0.669 (0.669)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:20 eps:0.1000]: Time 0.020 (0.038)  Total Loss 0.9489 (1.0210)  L1 Loss 0.0000 (0.0000)  CE 0.2250 (0.2351)  RCE 0.9489 (1.0210)  Uns 24.979 (25.930)  Dead 50.9 (50.1)  Alive 116.1 (116.0)  Tight 24.97917 (25.92978)  Bias -0.59152 (-0.69309)  Diff 13.42158 (14.40375)  Err 0.0729 (0.0644)  Rob Err 0.2708 (0.2916)  R 25.294  beta 0.667 (0.667)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.704858779907227\n",
      "layer 3 norm 9.38452434539795\n",
      "layer 5 norm 7.518311500549316\n",
      "Epoch time: 9.0662, Total time: 176.7909\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:20 eps:0.1000]: Time 0.003 (0.021)  Total Loss 0.4426 (0.9936)  L1 Loss 0.0000 (0.0000)  CE 0.0526 (0.2253)  RCE 0.4426 (0.9936)  Uns 22.688 (25.089)  Dead 52.4 (50.6)  Alive 116.9 (116.3)  Tight 22.68750 (25.08870)  Bias 0.55605 (-0.52356)  Diff 11.94485 (13.06748)  Err 0.0000 (0.0598)  Rob Err 0.2500 (0.2881)  R 22.415  beta 0.667 (0.667)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.704858779907227\n",
      "layer 3 norm 9.38452434539795\n",
      "layer 5 norm 7.518311500549316\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 21, learning rate [0.0005], epsilon 0.1 - 0.105\n",
      "[21:   0]: eps 0.100000  Time 0.110 (0.110)  Total Loss 1.0782 (1.0782)  L1 Loss 0.0000 (0.0000)  CE 0.2640 (0.2640)  RCE 1.0782 (1.0782)  Err 0.0859 (0.0859)  Rob Err 0.2812 (0.2812)  Uns 25.5 (25.5)  Dead 50.2 (50.2)  Alive 116.2 (116.2)  Tightness 25.54688 (25.54688)  Bias -1.79434 (-1.79434)  Diff 13.02754 (13.02754)  R 24.799  beta 0.667 (0.667)  kappa 1.000 (1.000)  \n",
      "[21:  50]: eps 0.101064  Time 0.031 (0.040)  Total Loss 1.2758 (1.0585)  L1 Loss 0.0000 (0.0000)  CE 0.3218 (0.2473)  RCE 1.2758 (1.0585)  Err 0.0977 (0.0668)  Rob Err 0.3828 (0.3057)  Uns 26.5 (25.7)  Dead 49.2 (50.2)  Alive 116.3 (116.1)  Tightness 26.45703 (25.66475)  Bias -1.44378 (-0.80888)  Diff 13.49026 (13.09575)  R 25.843  beta 0.663 (0.663)  kappa 1.000 (1.000)  \n",
      "[21: 100]: eps 0.102128  Time 0.032 (0.037)  Total Loss 1.1698 (1.0571)  L1 Loss 0.0000 (0.0000)  CE 0.2946 (0.2459)  RCE 1.1698 (1.0571)  Err 0.0703 (0.0664)  Rob Err 0.3438 (0.3019)  Uns 25.4 (25.8)  Dead 50.4 (50.1)  Alive 116.2 (116.1)  Tightness 25.40625 (25.80360)  Bias -1.55738 (-0.81819)  Diff 12.49069 (13.03326)  R 25.288  beta 0.660 (0.660)  kappa 1.000 (1.000)  \n",
      "[21: 150]: eps 0.103191  Time 0.044 (0.036)  Total Loss 0.9729 (1.0567)  L1 Loss 0.0000 (0.0000)  CE 0.1978 (0.2456)  RCE 0.9729 (1.0567)  Err 0.0469 (0.0662)  Rob Err 0.2578 (0.3015)  Uns 25.9 (25.8)  Dead 50.1 (50.0)  Alive 116.0 (116.2)  Tightness 25.90625 (25.83348)  Bias -0.12099 (-0.78074)  Diff 12.29072 (12.86290)  R 25.869  beta 0.656 (0.656)  kappa 1.000 (1.000)  \n",
      "[21: 200]: eps 0.104255  Time 0.051 (0.038)  Total Loss 1.0110 (1.0588)  L1 Loss 0.0000 (0.0000)  CE 0.2375 (0.2470)  RCE 1.0110 (1.0588)  Err 0.0586 (0.0666)  Rob Err 0.2891 (0.3030)  Uns 26.5 (25.9)  Dead 48.9 (50.0)  Alive 116.6 (116.1)  Tightness 26.48828 (25.85067)  Bias -1.41893 (-0.82815)  Diff 12.10794 (12.67841)  R 26.724  beta 0.652 (0.652)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:21 eps:0.1050]: Time 0.021 (0.039)  Total Loss 1.0569 (1.0629)  L1 Loss 0.0000 (0.0000)  CE 0.2596 (0.2488)  RCE 1.0569 (1.0629)  Uns 24.635 (25.859)  Dead 51.3 (50.0)  Alive 116.1 (116.1)  Tight 24.63542 (25.85872)  Bias -1.33355 (-0.86124)  Diff 11.49694 (12.57822)  Err 0.0729 (0.0670)  Rob Err 0.3125 (0.3043)  R 24.673  beta 0.650 (0.650)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.202890396118164\n",
      "layer 3 norm 9.760374069213867\n",
      "layer 5 norm 7.587275505065918\n",
      "Epoch time: 9.4613, Total time: 186.2522\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:21 eps:0.1050]: Time 0.003 (0.019)  Total Loss 0.4897 (1.0340)  L1 Loss 0.0000 (0.0000)  CE 0.0602 (0.2391)  RCE 0.4897 (1.0340)  Uns 22.938 (25.479)  Dead 52.7 (50.5)  Alive 116.4 (116.0)  Tight 22.93750 (25.47850)  Bias 0.35884 (-0.70114)  Diff 10.69658 (11.58017)  Err 0.0000 (0.0638)  Rob Err 0.1875 (0.2985)  R 23.085  beta 0.650 (0.650)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 20.202890396118164\n",
      "layer 3 norm 9.760374069213867\n",
      "layer 5 norm 7.587275505065918\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 22, learning rate [0.0005], epsilon 0.105 - 0.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:   0]: eps 0.105000  Time 0.110 (0.110)  Total Loss 1.0655 (1.0655)  L1 Loss 0.0000 (0.0000)  CE 0.2592 (0.2592)  RCE 1.0655 (1.0655)  Err 0.0664 (0.0664)  Rob Err 0.3086 (0.3086)  Uns 26.3 (26.3)  Dead 50.1 (50.1)  Alive 115.6 (115.6)  Tightness 26.30469 (26.30469)  Bias -0.60241 (-0.60241)  Diff 11.68780 (11.68780)  R 25.138  beta 0.650 (0.650)  kappa 1.000 (1.000)  \n",
      "[22:  50]: eps 0.106064  Time 0.055 (0.042)  Total Loss 1.1666 (1.0993)  L1 Loss 0.0000 (0.0000)  CE 0.2614 (0.2596)  RCE 1.1666 (1.0993)  Err 0.0742 (0.0718)  Rob Err 0.3555 (0.3155)  Uns 25.7 (25.8)  Dead 50.2 (50.0)  Alive 116.2 (116.2)  Tightness 25.67188 (25.80599)  Bias -0.80767 (-0.96806)  Diff 11.56540 (11.58317)  R 25.397  beta 0.646 (0.646)  kappa 1.000 (1.000)  \n",
      "[22: 100]: eps 0.107128  Time 0.035 (0.040)  Total Loss 0.9888 (1.0928)  L1 Loss 0.0000 (0.0000)  CE 0.2153 (0.2592)  RCE 0.9888 (1.0928)  Err 0.0547 (0.0704)  Rob Err 0.2617 (0.3117)  Uns 25.4 (25.9)  Dead 50.2 (49.9)  Alive 116.4 (116.2)  Tightness 25.44922 (25.86030)  Bias -1.00937 (-1.03905)  Diff 11.25991 (11.43131)  R 24.007  beta 0.643 (0.643)  kappa 1.000 (1.000)  \n",
      "[22: 150]: eps 0.108191  Time 0.053 (0.040)  Total Loss 0.9613 (1.0975)  L1 Loss 0.0000 (0.0000)  CE 0.2234 (0.2616)  RCE 0.9613 (1.0975)  Err 0.0625 (0.0705)  Rob Err 0.2656 (0.3124)  Uns 24.9 (25.8)  Dead 50.9 (49.9)  Alive 116.2 (116.3)  Tightness 24.89453 (25.80738)  Bias -0.29586 (-1.00754)  Diff 10.33713 (11.19129)  R 26.605  beta 0.639 (0.639)  kappa 1.000 (1.000)  \n",
      "[22: 200]: eps 0.109255  Time 0.049 (0.041)  Total Loss 1.1439 (1.1019)  L1 Loss 0.0000 (0.0000)  CE 0.2850 (0.2636)  RCE 1.1439 (1.1019)  Err 0.0781 (0.0714)  Rob Err 0.3320 (0.3146)  Uns 26.0 (25.8)  Dead 49.5 (49.9)  Alive 116.5 (116.3)  Tightness 25.96875 (25.80138)  Bias -1.08514 (-1.03792)  Diff 10.25639 (10.98701)  R 24.731  beta 0.636 (0.636)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:22 eps:0.1100]: Time 0.027 (0.042)  Total Loss 1.1928 (1.1013)  L1 Loss 0.0000 (0.0000)  CE 0.2817 (0.2632)  RCE 1.1928 (1.1013)  Uns 26.552 (25.829)  Dead 49.9 (49.9)  Alive 115.5 (116.3)  Tight 26.55208 (25.82883)  Bias -0.65387 (-1.08485)  Diff 10.45027 (10.89085)  Err 0.0938 (0.0708)  Rob Err 0.2708 (0.3140)  R 25.490  beta 0.633 (0.633)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.861318588256836\n",
      "layer 3 norm 10.016984939575195\n",
      "layer 5 norm 7.665863037109375\n",
      "Epoch time: 10.1532, Total time: 196.4054\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:22 eps:0.1100]: Time 0.005 (0.024)  Total Loss 0.5486 (1.0693)  L1 Loss 0.0000 (0.0000)  CE 0.0704 (0.2531)  RCE 0.5486 (1.0693)  Uns 23.062 (25.355)  Dead 52.1 (50.2)  Alive 116.9 (116.4)  Tight 23.06250 (25.35550)  Bias 0.08309 (-1.10412)  Diff 9.19321 (10.15898)  Err 0.0000 (0.0668)  Rob Err 0.2500 (0.3097)  R 23.108  beta 0.633 (0.633)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.861318588256836\n",
      "layer 3 norm 10.016984939575195\n",
      "layer 5 norm 7.665863037109375\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 23, learning rate [0.0005], epsilon 0.11 - 0.115\n",
      "[23:   0]: eps 0.110000  Time 0.132 (0.132)  Total Loss 1.1706 (1.1706)  L1 Loss 0.0000 (0.0000)  CE 0.2945 (0.2945)  RCE 1.1706 (1.1706)  Err 0.0742 (0.0742)  Rob Err 0.3398 (0.3398)  Uns 26.3 (26.3)  Dead 49.7 (49.7)  Alive 116.0 (116.0)  Tightness 26.31641 (26.31641)  Bias -0.16395 (-0.16395)  Diff 9.92663 (9.92663)  R 24.573  beta 0.633 (0.633)  kappa 1.000 (1.000)  \n",
      "[23:  50]: eps 0.111064  Time 0.037 (0.046)  Total Loss 1.2080 (1.1276)  L1 Loss 0.0000 (0.0000)  CE 0.3026 (0.2693)  RCE 1.2080 (1.1276)  Err 0.0898 (0.0724)  Rob Err 0.3398 (0.3217)  Uns 25.2 (25.9)  Dead 49.9 (49.8)  Alive 116.9 (116.3)  Tightness 25.23438 (25.91958)  Bias -1.67432 (-1.29012)  Diff 9.86719 (10.08841)  R 25.033  beta 0.630 (0.630)  kappa 1.000 (1.000)  \n",
      "[23: 100]: eps 0.112128  Time 0.035 (0.042)  Total Loss 0.9896 (1.1281)  L1 Loss 0.0000 (0.0000)  CE 0.2520 (0.2733)  RCE 0.9896 (1.1281)  Err 0.0352 (0.0732)  Rob Err 0.2500 (0.3217)  Uns 25.0 (25.9)  Dead 51.1 (49.9)  Alive 115.9 (116.3)  Tightness 24.99219 (25.86170)  Bias -1.59300 (-1.36064)  Diff 9.88610 (9.92973)  R 25.388  beta 0.626 (0.626)  kappa 1.000 (1.000)  \n",
      "[23: 150]: eps 0.113191  Time 0.036 (0.040)  Total Loss 1.0552 (1.1302)  L1 Loss 0.0000 (0.0000)  CE 0.2600 (0.2746)  RCE 1.0552 (1.1302)  Err 0.0664 (0.0738)  Rob Err 0.2852 (0.3223)  Uns 26.1 (25.9)  Dead 49.9 (49.8)  Alive 116.0 (116.3)  Tightness 26.06250 (25.89733)  Bias -0.42911 (-1.39915)  Diff 9.26461 (9.73122)  R 24.783  beta 0.623 (0.623)  kappa 1.000 (1.000)  \n",
      "[23: 200]: eps 0.114255  Time 0.032 (0.040)  Total Loss 1.3114 (1.1347)  L1 Loss 0.0000 (0.0000)  CE 0.3704 (0.2768)  RCE 1.3114 (1.1347)  Err 0.0859 (0.0744)  Rob Err 0.3359 (0.3238)  Uns 25.5 (25.9)  Dead 50.0 (49.8)  Alive 116.6 (116.3)  Tightness 25.45703 (25.89313)  Bias -0.87118 (-1.43567)  Diff 9.22154 (9.58207)  R 25.161  beta 0.619 (0.619)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:23 eps:0.1150]: Time 0.021 (0.040)  Total Loss 0.9320 (1.1376)  L1 Loss 0.0000 (0.0000)  CE 0.2440 (0.2773)  RCE 0.9320 (1.1376)  Uns 26.312 (25.886)  Dead 48.8 (49.8)  Alive 116.8 (116.3)  Tight 26.31250 (25.88608)  Bias -3.15129 (-1.45652)  Diff 9.14793 (9.47702)  Err 0.0729 (0.0747)  Rob Err 0.2812 (0.3247)  R 22.749  beta 0.617 (0.617)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.204608917236328\n",
      "layer 3 norm 10.28110122680664\n",
      "layer 5 norm 7.669816970825195\n",
      "Epoch time: 9.6718, Total time: 206.0772\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:23 eps:0.1150]: Time 0.003 (0.019)  Total Loss 0.6251 (1.1101)  L1 Loss 0.0000 (0.0000)  CE 0.0875 (0.2726)  RCE 0.6251 (1.1101)  Uns 23.500 (25.596)  Dead 52.0 (50.2)  Alive 116.5 (116.2)  Tight 23.50000 (25.59610)  Bias 0.06525 (-1.55951)  Diff 8.06320 (8.80757)  Err 0.0000 (0.0701)  Rob Err 0.3125 (0.3215)  R 22.895  beta 0.617 (0.617)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 19.204608917236328\n",
      "layer 3 norm 10.28110122680664\n",
      "layer 5 norm 7.669816970825195\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 24, learning rate [0.0005], epsilon 0.115 - 0.12\n",
      "[24:   0]: eps 0.115000  Time 0.111 (0.111)  Total Loss 1.0050 (1.0050)  L1 Loss 0.0000 (0.0000)  CE 0.1889 (0.1889)  RCE 1.0050 (1.0050)  Err 0.0469 (0.0469)  Rob Err 0.2930 (0.2930)  Uns 25.6 (25.6)  Dead 49.9 (49.9)  Alive 116.4 (116.4)  Tightness 25.64453 (25.64453)  Bias -0.65640 (-0.65640)  Diff 8.52483 (8.52483)  R 24.904  beta 0.617 (0.617)  kappa 1.000 (1.000)  \n",
      "[24:  50]: eps 0.116064  Time 0.032 (0.040)  Total Loss 1.0702 (1.1604)  L1 Loss 0.0000 (0.0000)  CE 0.2562 (0.2893)  RCE 1.0702 (1.1604)  Err 0.0586 (0.0777)  Rob Err 0.3047 (0.3314)  Uns 25.0 (25.9)  Dead 50.8 (49.8)  Alive 116.2 (116.3)  Tightness 24.98047 (25.94700)  Bias -2.00299 (-1.58729)  Diff 8.31966 (8.64078)  R 24.313  beta 0.613 (0.613)  kappa 1.000 (1.000)  \n",
      "[24: 100]: eps 0.117128  Time 0.050 (0.040)  Total Loss 1.2000 (1.1606)  L1 Loss 0.0000 (0.0000)  CE 0.2969 (0.2935)  RCE 1.2000 (1.1606)  Err 0.0781 (0.0791)  Rob Err 0.3711 (0.3319)  Uns 26.2 (26.0)  Dead 49.5 (49.8)  Alive 116.2 (116.2)  Tightness 26.23828 (25.98987)  Bias -1.07524 (-1.63662)  Diff 8.12951 (8.45883)  R 25.274  beta 0.610 (0.610)  kappa 1.000 (1.000)  \n",
      "[24: 150]: eps 0.118191  Time 0.053 (0.041)  Total Loss 1.2270 (1.1591)  L1 Loss 0.0000 (0.0000)  CE 0.3047 (0.2903)  RCE 1.2270 (1.1591)  Err 0.0898 (0.0773)  Rob Err 0.3555 (0.3317)  Uns 26.3 (26.0)  Dead 49.2 (49.7)  Alive 116.4 (116.3)  Tightness 26.32812 (26.02046)  Bias -2.41420 (-1.74455)  Diff 8.06264 (8.33518)  R 25.328  beta 0.606 (0.606)  kappa 1.000 (1.000)  \n",
      "[24: 200]: eps 0.119255  Time 0.034 (0.041)  Total Loss 1.1601 (1.1645)  L1 Loss 0.0000 (0.0000)  CE 0.2967 (0.2919)  RCE 1.1601 (1.1645)  Err 0.0781 (0.0780)  Rob Err 0.3359 (0.3329)  Uns 26.2 (26.0)  Dead 49.8 (49.7)  Alive 116.1 (116.2)  Tightness 26.17188 (26.01075)  Bias -2.54621 (-1.78205)  Diff 7.52073 (8.19174)  R 22.884  beta 0.602 (0.602)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:24 eps:0.1200]: Time 0.023 (0.041)  Total Loss 1.1395 (1.1695)  L1 Loss 0.0000 (0.0000)  CE 0.2651 (0.2934)  RCE 1.1395 (1.1695)  Uns 25.458 (26.005)  Dead 50.2 (49.7)  Alive 116.3 (116.3)  Tight 25.45833 (26.00497)  Bias -1.72249 (-1.80465)  Diff 7.25815 (8.10215)  Err 0.0625 (0.0785)  Rob Err 0.3750 (0.3351)  R 23.176  beta 0.600 (0.600)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 18.60214614868164\n",
      "layer 3 norm 10.530204772949219\n",
      "layer 5 norm 7.631095886230469\n",
      "Epoch time: 9.8337, Total time: 215.9109\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL RESULT epoch:24 eps:0.1200]: Time 0.003 (0.021)  Total Loss 0.6875 (1.1344)  L1 Loss 0.0000 (0.0000)  CE 0.0912 (0.2862)  RCE 0.6875 (1.1344)  Uns 22.938 (25.817)  Dead 52.5 (50.2)  Alive 116.5 (116.0)  Tight 22.93750 (25.81710)  Bias 0.19598 (-1.69032)  Diff 7.00226 (7.32839)  Err 0.0000 (0.0771)  Rob Err 0.2500 (0.3273)  R 22.245  beta 0.600 (0.600)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 18.60214614868164\n",
      "layer 3 norm 10.530204772949219\n",
      "layer 5 norm 7.631095886230469\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 25, learning rate [0.0005], epsilon 0.12 - 0.125\n",
      "[25:   0]: eps 0.120000  Time 0.111 (0.111)  Total Loss 1.1861 (1.1861)  L1 Loss 0.0000 (0.0000)  CE 0.2581 (0.2581)  RCE 1.1861 (1.1861)  Err 0.0664 (0.0664)  Rob Err 0.3203 (0.3203)  Uns 26.5 (26.5)  Dead 49.5 (49.5)  Alive 116.0 (116.0)  Tightness 26.48047 (26.48047)  Bias -3.02226 (-3.02226)  Diff 7.65648 (7.65648)  R 23.144  beta 0.600 (0.600)  kappa 1.000 (1.000)  \n",
      "[25:  50]: eps 0.121064  Time 0.039 (0.049)  Total Loss 1.2392 (1.1890)  L1 Loss 0.0000 (0.0000)  CE 0.3342 (0.3073)  RCE 1.2392 (1.1890)  Err 0.0859 (0.0837)  Rob Err 0.3633 (0.3395)  Uns 25.4 (26.1)  Dead 50.3 (49.7)  Alive 116.3 (116.2)  Tightness 25.41016 (26.09727)  Bias -1.08807 (-1.95158)  Diff 7.37900 (7.29938)  R 24.190  beta 0.596 (0.596)  kappa 1.000 (1.000)  \n",
      "[25: 100]: eps 0.122128  Time 0.044 (0.044)  Total Loss 1.1989 (1.1918)  L1 Loss 0.0000 (0.0000)  CE 0.3128 (0.3075)  RCE 1.1989 (1.1918)  Err 0.0898 (0.0827)  Rob Err 0.3555 (0.3404)  Uns 26.9 (26.1)  Dead 49.8 (49.8)  Alive 115.3 (116.2)  Tightness 26.88281 (26.09321)  Bias -1.90660 (-2.02179)  Diff 7.03114 (7.13419)  R 24.442  beta 0.593 (0.593)  kappa 1.000 (1.000)  \n",
      "[25: 150]: eps 0.123191  Time 0.041 (0.043)  Total Loss 1.2975 (1.1883)  L1 Loss 0.0000 (0.0000)  CE 0.3643 (0.3070)  RCE 1.2975 (1.1883)  Err 0.1016 (0.0824)  Rob Err 0.3594 (0.3385)  Uns 26.5 (26.2)  Dead 49.9 (49.8)  Alive 115.6 (116.1)  Tightness 26.52344 (26.18033)  Bias -2.17302 (-2.06105)  Diff 6.76505 (7.01304)  R 23.916  beta 0.589 (0.589)  kappa 1.000 (1.000)  \n",
      "[25: 200]: eps 0.124255  Time 0.032 (0.043)  Total Loss 1.3499 (1.1935)  L1 Loss 0.0000 (0.0000)  CE 0.3953 (0.3097)  RCE 1.3499 (1.1935)  Err 0.1016 (0.0828)  Rob Err 0.3789 (0.3410)  Uns 26.1 (26.3)  Dead 50.0 (49.7)  Alive 116.0 (116.0)  Tightness 26.09375 (26.25045)  Bias -1.64214 (-2.11335)  Diff 6.42545 (6.91402)  R 23.650  beta 0.586 (0.586)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:25 eps:0.1250]: Time 0.025 (0.042)  Total Loss 0.9595 (1.1962)  L1 Loss 0.0000 (0.0000)  CE 0.1940 (0.3108)  RCE 0.9595 (1.1962)  Uns 26.052 (26.276)  Dead 50.3 (49.8)  Alive 115.7 (116.0)  Tight 26.05208 (26.27582)  Bias -3.01317 (-2.10981)  Diff 6.21563 (6.83923)  Err 0.0521 (0.0833)  Rob Err 0.3125 (0.3415)  R 23.911  beta 0.583 (0.583)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 17.941905975341797\n",
      "layer 3 norm 10.615813255310059\n",
      "layer 5 norm 7.621801853179932\n",
      "Epoch time: 10.1726, Total time: 226.0835\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:25 eps:0.1250]: Time 0.003 (0.016)  Total Loss 0.7701 (1.1622)  L1 Loss 0.0000 (0.0000)  CE 0.1061 (0.3007)  RCE 0.7701 (1.1622)  Uns 22.250 (26.230)  Dead 53.2 (50.1)  Alive 116.5 (115.7)  Tight 22.25000 (26.22950)  Bias -0.05859 (-2.10964)  Diff 5.87698 (6.23600)  Err 0.0000 (0.0787)  Rob Err 0.3125 (0.3348)  R 21.491  beta 0.583 (0.583)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 17.941905975341797\n",
      "layer 3 norm 10.615813255310059\n",
      "layer 5 norm 7.621801853179932\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 26, learning rate [0.0005], epsilon 0.125 - 0.13\n",
      "[26:   0]: eps 0.125000  Time 0.109 (0.109)  Total Loss 1.2869 (1.2869)  L1 Loss 0.0000 (0.0000)  CE 0.3469 (0.3469)  RCE 1.2869 (1.2869)  Err 0.1016 (0.1016)  Rob Err 0.3672 (0.3672)  Uns 26.9 (26.9)  Dead 49.5 (49.5)  Alive 115.6 (115.6)  Tightness 26.91016 (26.91016)  Bias -1.47729 (-1.47729)  Diff 6.44387 (6.44387)  R 24.031  beta 0.583 (0.583)  kappa 1.000 (1.000)  \n",
      "[26:  50]: eps 0.126064  Time 0.033 (0.038)  Total Loss 1.3101 (1.1929)  L1 Loss 0.0000 (0.0000)  CE 0.3865 (0.3113)  RCE 1.3101 (1.1929)  Err 0.1289 (0.0806)  Rob Err 0.3945 (0.3441)  Uns 25.9 (26.6)  Dead 50.4 (49.7)  Alive 115.6 (115.7)  Tightness 25.91797 (26.58555)  Bias -3.45775 (-2.12859)  Diff 6.13613 (6.13337)  R 24.354  beta 0.580 (0.580)  kappa 1.000 (1.000)  \n",
      "[26: 100]: eps 0.127128  Time 0.031 (0.036)  Total Loss 1.1592 (1.2127)  L1 Loss 0.0000 (0.0000)  CE 0.3060 (0.3211)  RCE 1.1592 (1.2127)  Err 0.0820 (0.0861)  Rob Err 0.3164 (0.3458)  Uns 26.1 (26.6)  Dead 50.1 (49.8)  Alive 115.8 (115.6)  Tightness 26.14844 (26.55558)  Bias -2.91873 (-2.25240)  Diff 5.79679 (6.04986)  R 23.342  beta 0.576 (0.576)  kappa 1.000 (1.000)  \n",
      "[26: 150]: eps 0.128191  Time 0.034 (0.036)  Total Loss 1.1915 (1.2191)  L1 Loss 0.0000 (0.0000)  CE 0.3249 (0.3249)  RCE 1.1915 (1.2191)  Err 0.0898 (0.0868)  Rob Err 0.3398 (0.3474)  Uns 26.4 (26.5)  Dead 50.0 (49.9)  Alive 115.6 (115.6)  Tightness 26.37891 (26.54879)  Bias -3.36742 (-2.39863)  Diff 5.71690 (5.91810)  R 23.652  beta 0.573 (0.573)  kappa 1.000 (1.000)  \n",
      "[26: 200]: eps 0.129255  Time 0.035 (0.036)  Total Loss 1.2905 (1.2198)  L1 Loss 0.0000 (0.0000)  CE 0.3462 (0.3265)  RCE 1.2905 (1.2198)  Err 0.1016 (0.0881)  Rob Err 0.3867 (0.3477)  Uns 26.4 (26.6)  Dead 50.1 (49.9)  Alive 115.5 (115.5)  Tightness 26.44531 (26.58318)  Bias -3.55812 (-2.44462)  Diff 5.18659 (5.82651)  R 23.308  beta 0.569 (0.569)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:26 eps:0.1300]: Time 0.018 (0.036)  Total Loss 1.3320 (1.2217)  L1 Loss 0.0000 (0.0000)  CE 0.3811 (0.3279)  RCE 1.3320 (1.2217)  Uns 27.542 (26.611)  Dead 49.9 (49.9)  Alive 114.6 (115.5)  Tight 27.54167 (26.61120)  Bias -2.18701 (-2.49888)  Diff 5.02477 (5.77198)  Err 0.1146 (0.0882)  Rob Err 0.4271 (0.3487)  R 23.567  beta 0.567 (0.567)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 17.165449142456055\n",
      "layer 3 norm 10.58775520324707\n",
      "layer 5 norm 7.649231910705566\n",
      "Epoch time: 8.6120, Total time: 234.6954\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:26 eps:0.1300]: Time 0.003 (0.018)  Total Loss 0.8402 (1.1843)  L1 Loss 0.0000 (0.0000)  CE 0.1313 (0.3209)  RCE 0.8402 (1.1843)  Uns 21.750 (26.285)  Dead 53.7 (50.4)  Alive 116.6 (115.3)  Tight 21.75000 (26.28500)  Bias -0.38935 (-2.51450)  Diff 4.86679 (5.17289)  Err 0.0000 (0.0833)  Rob Err 0.3125 (0.3374)  R 20.878  beta 0.567 (0.567)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 17.165449142456055\n",
      "layer 3 norm 10.58775520324707\n",
      "layer 5 norm 7.649231910705566\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 27, learning rate [0.0005], epsilon 0.13 - 0.135\n",
      "[27:   0]: eps 0.130000  Time 0.115 (0.115)  Total Loss 1.0819 (1.0819)  L1 Loss 0.0000 (0.0000)  CE 0.2754 (0.2754)  RCE 1.0819 (1.0819)  Err 0.0586 (0.0586)  Rob Err 0.3281 (0.3281)  Uns 26.3 (26.3)  Dead 50.0 (50.0)  Alive 115.7 (115.7)  Tightness 26.28125 (26.28125)  Bias -3.96333 (-3.96333)  Diff 5.04443 (5.04443)  R 23.629  beta 0.567 (0.567)  kappa 1.000 (1.000)  \n",
      "[27:  50]: eps 0.131064  Time 0.033 (0.043)  Total Loss 1.2509 (1.2447)  L1 Loss 0.0000 (0.0000)  CE 0.3249 (0.3449)  RCE 1.2509 (1.2447)  Err 0.0820 (0.0917)  Rob Err 0.3438 (0.3539)  Uns 27.3 (26.8)  Dead 49.4 (49.9)  Alive 115.3 (115.3)  Tightness 27.31250 (26.83410)  Bias -2.92679 (-2.83835)  Diff 5.23713 (5.11274)  R 23.208  beta 0.563 (0.563)  kappa 1.000 (1.000)  \n",
      "[27: 100]: eps 0.132128  Time 0.044 (0.039)  Total Loss 1.0103 (1.2380)  L1 Loss 0.0000 (0.0000)  CE 0.2307 (0.3429)  RCE 1.0103 (1.2380)  Err 0.0312 (0.0925)  Rob Err 0.2891 (0.3526)  Uns 26.9 (26.9)  Dead 50.4 (49.9)  Alive 114.7 (115.2)  Tightness 26.94922 (26.87218)  Bias -2.75993 (-2.78292)  Diff 4.65312 (5.01774)  R 22.777  beta 0.560 (0.560)  kappa 1.000 (1.000)  \n",
      "[27: 150]: eps 0.133191  Time 0.031 (0.037)  Total Loss 1.1981 (1.2393)  L1 Loss 0.0000 (0.0000)  CE 0.3294 (0.3448)  RCE 1.1981 (1.2393)  Err 0.0742 (0.0935)  Rob Err 0.3359 (0.3529)  Uns 26.5 (27.0)  Dead 50.2 (49.9)  Alive 115.3 (115.2)  Tightness 26.46484 (26.95874)  Bias -2.82425 (-2.85658)  Diff 4.75266 (4.94163)  R 24.097  beta 0.556 (0.556)  kappa 1.000 (1.000)  \n",
      "[27: 200]: eps 0.134255  Time 0.034 (0.036)  Total Loss 1.1886 (1.2434)  L1 Loss 0.0000 (0.0000)  CE 0.3277 (0.3482)  RCE 1.1886 (1.2434)  Err 0.0820 (0.0941)  Rob Err 0.3281 (0.3541)  Uns 27.7 (27.0)  Dead 49.7 (49.9)  Alive 114.6 (115.1)  Tightness 27.73828 (26.97542)  Bias -2.93951 (-2.87261)  Diff 4.65735 (4.84943)  R 24.144  beta 0.552 (0.552)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL RESULT epoch:27 eps:0.1350]: Time 0.016 (0.036)  Total Loss 1.3783 (1.2438)  L1 Loss 0.0000 (0.0000)  CE 0.3324 (0.3480)  RCE 1.3783 (1.2438)  Uns 27.927 (27.012)  Dead 48.3 (49.9)  Alive 115.8 (115.1)  Tight 27.92708 (27.01175)  Bias -2.66838 (-2.88450)  Diff 4.37002 (4.78770)  Err 0.0833 (0.0935)  Rob Err 0.4271 (0.3540)  R 22.685  beta 0.550 (0.550)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 16.452974319458008\n",
      "layer 3 norm 10.428391456604004\n",
      "layer 5 norm 7.711224555969238\n",
      "Epoch time: 8.6795, Total time: 243.3749\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:27 eps:0.1350]: Time 0.003 (0.017)  Total Loss 0.8988 (1.2100)  L1 Loss 0.0000 (0.0000)  CE 0.1515 (0.3412)  RCE 0.8988 (1.2100)  Uns 23.000 (26.676)  Dead 53.1 (50.4)  Alive 115.9 (114.9)  Tight 23.00000 (26.67560)  Bias -0.49169 (-2.95861)  Diff 4.14321 (4.33975)  Err 0.0000 (0.0919)  Rob Err 0.2500 (0.3491)  R 20.344  beta 0.550 (0.550)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 16.452974319458008\n",
      "layer 3 norm 10.428391456604004\n",
      "layer 5 norm 7.711224555969238\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 28, learning rate [0.0005], epsilon 0.135 - 0.14\n",
      "[28:   0]: eps 0.135000  Time 0.115 (0.115)  Total Loss 1.3022 (1.3022)  L1 Loss 0.0000 (0.0000)  CE 0.3818 (0.3818)  RCE 1.3022 (1.3022)  Err 0.1133 (0.1133)  Rob Err 0.3750 (0.3750)  Uns 26.6 (26.6)  Dead 50.1 (50.1)  Alive 115.2 (115.2)  Tightness 26.64062 (26.64062)  Bias -3.41309 (-3.41309)  Diff 4.30683 (4.30683)  R 23.222  beta 0.550 (0.550)  kappa 1.000 (1.000)  \n",
      "[28:  50]: eps 0.136064  Time 0.032 (0.041)  Total Loss 1.3147 (1.2587)  L1 Loss 0.0000 (0.0000)  CE 0.3953 (0.3620)  RCE 1.3147 (1.2587)  Err 0.1211 (0.0980)  Rob Err 0.3320 (0.3596)  Uns 27.8 (27.4)  Dead 49.2 (49.8)  Alive 115.0 (114.8)  Tightness 27.75391 (27.39744)  Bias -1.88630 (-3.02915)  Diff 4.44389 (4.32618)  R 22.715  beta 0.546 (0.546)  kappa 1.000 (1.000)  \n",
      "[28: 100]: eps 0.137128  Time 0.034 (0.041)  Total Loss 1.3544 (1.2689)  L1 Loss 0.0000 (0.0000)  CE 0.4383 (0.3674)  RCE 1.3544 (1.2689)  Err 0.1406 (0.0991)  Rob Err 0.3906 (0.3641)  Uns 27.7 (27.4)  Dead 49.3 (49.8)  Alive 115.0 (114.8)  Tightness 27.68359 (27.40706)  Bias -3.76156 (-3.07756)  Diff 3.95612 (4.20373)  R 23.464  beta 0.543 (0.543)  kappa 1.000 (1.000)  \n",
      "[28: 150]: eps 0.138191  Time 0.033 (0.039)  Total Loss 1.3485 (1.2684)  L1 Loss 0.0000 (0.0000)  CE 0.4122 (0.3678)  RCE 1.3485 (1.2684)  Err 0.1133 (0.0989)  Rob Err 0.4102 (0.3629)  Uns 28.3 (27.5)  Dead 48.9 (49.8)  Alive 114.8 (114.7)  Tightness 28.25781 (27.49108)  Bias -4.68023 (-3.18335)  Diff 3.56059 (4.09942)  R 22.231  beta 0.539 (0.539)  kappa 1.000 (1.000)  \n",
      "[28: 200]: eps 0.139255  Time 0.034 (0.038)  Total Loss 1.1420 (1.2664)  L1 Loss 0.0000 (0.0000)  CE 0.3160 (0.3682)  RCE 1.1420 (1.2664)  Err 0.0664 (0.0987)  Rob Err 0.3281 (0.3621)  Uns 27.9 (27.5)  Dead 49.7 (49.8)  Alive 114.4 (114.6)  Tightness 27.91797 (27.53735)  Bias -4.96947 (-3.22194)  Diff 3.43754 (4.00661)  R 22.170  beta 0.536 (0.536)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:28 eps:0.1400]: Time 0.017 (0.038)  Total Loss 1.4215 (1.2654)  L1 Loss 0.0000 (0.0000)  CE 0.4228 (0.3682)  RCE 1.4215 (1.2654)  Uns 27.823 (27.616)  Dead 49.3 (49.8)  Alive 114.9 (114.6)  Tight 27.82292 (27.61558)  Bias -4.38634 (-3.22689)  Diff 3.37801 (3.95758)  Err 0.1146 (0.0991)  Rob Err 0.3958 (0.3614)  R 23.852  beta 0.533 (0.533)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 15.594072341918945\n",
      "layer 3 norm 10.190656661987305\n",
      "layer 5 norm 7.857295036315918\n",
      "Epoch time: 9.0823, Total time: 252.4573\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:28 eps:0.1400]: Time 0.003 (0.016)  Total Loss 0.9503 (1.2299)  L1 Loss 0.0000 (0.0000)  CE 0.1910 (0.3625)  RCE 0.9503 (1.2299)  Uns 24.438 (27.477)  Dead 52.1 (50.3)  Alive 115.4 (114.2)  Tight 24.43750 (27.47710)  Bias -0.65054 (-3.22610)  Diff 3.58858 (3.60092)  Err 0.0000 (0.0966)  Rob Err 0.2500 (0.3508)  R 19.607  beta 0.533 (0.533)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 15.594072341918945\n",
      "layer 3 norm 10.190656661987305\n",
      "layer 5 norm 7.857295036315918\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 29, learning rate [0.0005], epsilon 0.14 - 0.145\n",
      "[29:   0]: eps 0.140000  Time 0.089 (0.089)  Total Loss 1.3793 (1.3793)  L1 Loss 0.0000 (0.0000)  CE 0.4123 (0.4123)  RCE 1.3793 (1.3793)  Err 0.1133 (0.1133)  Rob Err 0.4102 (0.4102)  Uns 27.9 (27.9)  Dead 49.5 (49.5)  Alive 114.6 (114.6)  Tightness 27.90625 (27.90625)  Bias -3.89550 (-3.89550)  Diff 3.92406 (3.92406)  R 22.073  beta 0.533 (0.533)  kappa 1.000 (1.000)  \n",
      "[29:  50]: eps 0.141064  Time 0.031 (0.040)  Total Loss 1.3962 (1.2910)  L1 Loss 0.0000 (0.0000)  CE 0.4081 (0.3842)  RCE 1.3962 (1.2910)  Err 0.1250 (0.1028)  Rob Err 0.4141 (0.3691)  Uns 28.5 (28.0)  Dead 49.1 (49.8)  Alive 114.4 (114.2)  Tightness 28.51172 (28.00329)  Bias -4.54244 (-3.44139)  Diff 3.30900 (3.53006)  R 21.388  beta 0.530 (0.530)  kappa 1.000 (1.000)  \n",
      "[29: 100]: eps 0.142128  Time 0.034 (0.037)  Total Loss 1.3672 (1.2901)  L1 Loss 0.0000 (0.0000)  CE 0.4071 (0.3880)  RCE 1.3672 (1.2901)  Err 0.0977 (0.1044)  Rob Err 0.4375 (0.3678)  Uns 28.4 (28.0)  Dead 49.6 (49.9)  Alive 114.0 (114.1)  Tightness 28.40234 (28.04602)  Bias -3.45273 (-3.41721)  Diff 3.07523 (3.42073)  R 22.152  beta 0.526 (0.526)  kappa 1.000 (1.000)  \n",
      "[29: 150]: eps 0.143191  Time 0.033 (0.036)  Total Loss 1.1263 (1.2837)  L1 Loss 0.0000 (0.0000)  CE 0.3197 (0.3864)  RCE 1.1263 (1.2837)  Err 0.0977 (0.1047)  Rob Err 0.3477 (0.3676)  Uns 27.2 (28.1)  Dead 50.5 (49.8)  Alive 114.3 (114.1)  Tightness 27.16016 (28.08736)  Bias -2.99883 (-3.48629)  Diff 3.27568 (3.34404)  R 22.855  beta 0.523 (0.523)  kappa 1.000 (1.000)  \n",
      "[29: 200]: eps 0.144255  Time 0.035 (0.036)  Total Loss 1.4595 (1.2870)  L1 Loss 0.0000 (0.0000)  CE 0.4516 (0.3900)  RCE 1.4595 (1.2870)  Err 0.1289 (0.1063)  Rob Err 0.3828 (0.3669)  Uns 28.7 (28.1)  Dead 49.2 (49.8)  Alive 114.1 (114.0)  Tightness 28.67578 (28.11056)  Bias -3.72537 (-3.51140)  Diff 3.26495 (3.27607)  R 23.374  beta 0.519 (0.519)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:29 eps:0.1450]: Time 0.017 (0.035)  Total Loss 1.2005 (1.2834)  L1 Loss 0.0000 (0.0000)  CE 0.3670 (0.3884)  RCE 1.2005 (1.2834)  Uns 28.073 (28.133)  Dead 50.1 (49.8)  Alive 113.8 (114.0)  Tight 28.07292 (28.13292)  Bias -2.51028 (-3.53281)  Diff 3.08481 (3.22889)  Err 0.0833 (0.1056)  Rob Err 0.3021 (0.3654)  R 22.098  beta 0.517 (0.517)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 14.893698692321777\n",
      "layer 3 norm 9.953548431396484\n",
      "layer 5 norm 8.020135879516602\n",
      "Epoch time: 8.5259, Total time: 260.9831\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:29 eps:0.1450]: Time 0.004 (0.017)  Total Loss 1.0311 (1.2470)  L1 Loss 0.0000 (0.0000)  CE 0.2306 (0.3830)  RCE 1.0311 (1.2470)  Uns 23.500 (27.809)  Dead 53.1 (50.4)  Alive 115.4 (113.8)  Tight 23.50000 (27.80950)  Bias -1.04253 (-3.51704)  Diff 2.91056 (2.96349)  Err 0.0000 (0.1031)  Rob Err 0.2500 (0.3542)  R 19.062  beta 0.517 (0.517)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 14.893698692321777\n",
      "layer 3 norm 9.953548431396484\n",
      "layer 5 norm 8.020135879516602\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 30, learning rate [0.0005], epsilon 0.145 - 0.15\n",
      "[30:   0]: eps 0.145000  Time 0.101 (0.101)  Total Loss 1.2034 (1.2034)  L1 Loss 0.0000 (0.0000)  CE 0.3685 (0.3685)  RCE 1.2034 (1.2034)  Err 0.0977 (0.0977)  Rob Err 0.3203 (0.3203)  Uns 27.8 (27.8)  Dead 50.3 (50.3)  Alive 113.9 (113.9)  Tightness 27.81641 (27.81641)  Bias -3.31897 (-3.31897)  Diff 3.13503 (3.13503)  R 22.478  beta 0.517 (0.517)  kappa 1.000 (1.000)  \n",
      "[30:  50]: eps 0.146064  Time 0.033 (0.039)  Total Loss 1.3603 (1.2889)  L1 Loss 0.0000 (0.0000)  CE 0.4756 (0.3937)  RCE 1.3603 (1.2889)  Err 0.1250 (0.1073)  Rob Err 0.3750 (0.3677)  Uns 27.9 (28.4)  Dead 49.9 (49.7)  Alive 114.1 (113.9)  Tightness 27.91016 (28.40962)  Bias -3.03182 (-3.74363)  Diff 2.95091 (2.88738)  R 22.582  beta 0.513 (0.513)  kappa 1.000 (1.000)  \n",
      "[30: 100]: eps 0.147128  Time 0.033 (0.037)  Total Loss 1.4720 (1.2899)  L1 Loss 0.0000 (0.0000)  CE 0.5002 (0.3950)  RCE 1.4720 (1.2899)  Err 0.1523 (0.1078)  Rob Err 0.3711 (0.3676)  Uns 29.7 (28.5)  Dead 48.6 (49.7)  Alive 113.7 (113.8)  Tightness 29.67969 (28.46589)  Bias -4.84518 (-3.80809)  Diff 2.64405 (2.79737)  R 22.644  beta 0.510 (0.510)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30: 150]: eps 0.148191  Time 0.044 (0.036)  Total Loss 1.1565 (1.2940)  L1 Loss 0.0000 (0.0000)  CE 0.3693 (0.4022)  RCE 1.1565 (1.2940)  Err 0.1016 (0.1102)  Rob Err 0.3359 (0.3670)  Uns 29.0 (28.4)  Dead 49.6 (49.8)  Alive 113.4 (113.8)  Tightness 28.98047 (28.44456)  Bias -3.97932 (-3.85474)  Diff 2.27056 (2.73326)  R 21.610  beta 0.506 (0.506)  kappa 1.000 (1.000)  \n",
      "[30: 200]: eps 0.149255  Time 0.032 (0.036)  Total Loss 1.3386 (1.2986)  L1 Loss 0.0000 (0.0000)  CE 0.4382 (0.4068)  RCE 1.3386 (1.2986)  Err 0.1016 (0.1112)  Rob Err 0.3750 (0.3681)  Uns 28.4 (28.5)  Dead 49.8 (49.8)  Alive 113.8 (113.7)  Tightness 28.42188 (28.49850)  Bias -2.97995 (-3.90585)  Diff 2.44299 (2.65849)  R 21.281  beta 0.502 (0.502)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:30 eps:0.1500]: Time 0.017 (0.035)  Total Loss 1.3204 (1.3003)  L1 Loss 0.0000 (0.0000)  CE 0.4274 (0.4088)  RCE 1.3204 (1.3003)  Uns 27.771 (28.524)  Dead 50.9 (49.8)  Alive 113.3 (113.7)  Tight 27.77083 (28.52358)  Bias -3.06276 (-3.91411)  Diff 2.49027 (2.61665)  Err 0.1042 (0.1118)  Rob Err 0.3750 (0.3686)  R 19.097  beta 0.500 (0.500)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 14.148096084594727\n",
      "layer 3 norm 9.622998237609863\n",
      "layer 5 norm 8.134032249450684\n",
      "Epoch time: 8.5180, Total time: 269.5012\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:30 eps:0.1500]: Time 0.003 (0.017)  Total Loss 1.0839 (1.2590)  L1 Loss 0.0000 (0.0000)  CE 0.2604 (0.4017)  RCE 1.0839 (1.2590)  Uns 23.750 (28.120)  Dead 53.1 (50.3)  Alive 115.1 (113.6)  Tight 23.75000 (28.11970)  Bias -1.17821 (-3.97953)  Diff 2.41810 (2.33352)  Err 0.0625 (0.1063)  Rob Err 0.2500 (0.3555)  R 18.263  beta 0.500 (0.500)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 14.148096084594727\n",
      "layer 3 norm 9.622998237609863\n",
      "layer 5 norm 8.134032249450684\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 31, learning rate [0.0005], epsilon 0.15 - 0.155\n",
      "[31:   0]: eps 0.150000  Time 0.126 (0.126)  Total Loss 1.2364 (1.2364)  L1 Loss 0.0000 (0.0000)  CE 0.4035 (0.4035)  RCE 1.2364 (1.2364)  Err 0.0977 (0.0977)  Rob Err 0.3555 (0.3555)  Uns 28.6 (28.6)  Dead 49.6 (49.6)  Alive 113.8 (113.8)  Tightness 28.64844 (28.64844)  Bias -5.75099 (-5.75099)  Diff 2.12458 (2.12458)  R 20.507  beta 0.500 (0.500)  kappa 1.000 (1.000)  \n",
      "[31:  50]: eps 0.151064  Time 0.033 (0.041)  Total Loss 1.2482 (1.3013)  L1 Loss 0.0000 (0.0000)  CE 0.4222 (0.4226)  RCE 1.2482 (1.3013)  Err 0.1016 (0.1142)  Rob Err 0.3516 (0.3706)  Uns 28.8 (28.8)  Dead 49.8 (49.6)  Alive 113.5 (113.6)  Tightness 28.77344 (28.81242)  Bias -4.68923 (-4.14788)  Diff 2.12200 (2.25727)  R 22.424  beta 0.496 (0.496)  kappa 1.000 (1.000)  \n",
      "[31: 100]: eps 0.152128  Time 0.031 (0.044)  Total Loss 1.3532 (1.3033)  L1 Loss 0.0000 (0.0000)  CE 0.4642 (0.4208)  RCE 1.3532 (1.3033)  Err 0.1289 (0.1148)  Rob Err 0.3828 (0.3708)  Uns 28.7 (28.9)  Dead 49.7 (49.6)  Alive 113.6 (113.5)  Tightness 28.70312 (28.87643)  Bias -2.82773 (-4.29594)  Diff 1.96882 (2.25174)  R 21.159  beta 0.493 (0.493)  kappa 1.000 (1.000)  \n",
      "[31: 150]: eps 0.153191  Time 0.032 (0.043)  Total Loss 1.1592 (1.3099)  L1 Loss 0.0000 (0.0000)  CE 0.3216 (0.4249)  RCE 1.1592 (1.3099)  Err 0.0703 (0.1148)  Rob Err 0.3477 (0.3703)  Uns 29.6 (28.9)  Dead 49.6 (49.6)  Alive 112.8 (113.4)  Tightness 29.57031 (28.93993)  Bias -4.09393 (-4.28186)  Diff 2.01635 (2.19088)  R 23.038  beta 0.489 (0.489)  kappa 1.000 (1.000)  \n",
      "[31: 200]: eps 0.154255  Time 0.034 (0.041)  Total Loss 1.2487 (1.3146)  L1 Loss 0.0000 (0.0000)  CE 0.4295 (0.4274)  RCE 1.2487 (1.3146)  Err 0.1367 (0.1162)  Rob Err 0.3438 (0.3727)  Uns 29.1 (29.0)  Dead 49.7 (49.6)  Alive 113.1 (113.4)  Tightness 29.14453 (28.97561)  Bias -3.88486 (-4.25280)  Diff 1.81846 (2.14141)  R 23.598  beta 0.486 (0.486)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:31 eps:0.1550]: Time 0.015 (0.040)  Total Loss 1.3000 (1.3145)  L1 Loss 0.0000 (0.0000)  CE 0.3999 (0.4279)  RCE 1.3000 (1.3145)  Uns 29.396 (28.997)  Dead 49.4 (49.6)  Alive 113.2 (113.4)  Tight 29.39583 (28.99678)  Bias -3.60784 (-4.22302)  Diff 2.09329 (2.10573)  Err 0.0625 (0.1163)  Rob Err 0.3438 (0.3719)  R 21.084  beta 0.483 (0.483)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 13.487666130065918\n",
      "layer 3 norm 9.453872680664062\n",
      "layer 5 norm 8.20881175994873\n",
      "Epoch time: 9.5092, Total time: 279.0104\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:31 eps:0.1550]: Time 0.003 (0.017)  Total Loss 1.1216 (1.2720)  L1 Loss 0.0000 (0.0000)  CE 0.2963 (0.4206)  RCE 1.1216 (1.2720)  Uns 25.562 (28.700)  Dead 52.8 (50.2)  Alive 113.6 (113.1)  Tight 25.56250 (28.70010)  Bias -1.33911 (-4.20958)  Diff 1.81926 (1.85364)  Err 0.0625 (0.1129)  Rob Err 0.2500 (0.3606)  R 17.500  beta 0.483 (0.483)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 13.487666130065918\n",
      "layer 3 norm 9.453872680664062\n",
      "layer 5 norm 8.20881175994873\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 32, learning rate [0.0005], epsilon 0.155 - 0.16\n",
      "[32:   0]: eps 0.155000  Time 0.104 (0.104)  Total Loss 1.2745 (1.2745)  L1 Loss 0.0000 (0.0000)  CE 0.4338 (0.4338)  RCE 1.2745 (1.2745)  Err 0.1250 (0.1250)  Rob Err 0.3711 (0.3711)  Uns 29.3 (29.3)  Dead 49.9 (49.9)  Alive 112.8 (112.8)  Tightness 29.30859 (29.30859)  Bias -5.94587 (-5.94587)  Diff 1.90375 (1.90375)  R 21.549  beta 0.483 (0.483)  kappa 1.000 (1.000)  \n",
      "[32:  50]: eps 0.156064  Time 0.032 (0.040)  Total Loss 1.4001 (1.3182)  L1 Loss 0.0000 (0.0000)  CE 0.4760 (0.4421)  RCE 1.4001 (1.3182)  Err 0.1328 (0.1201)  Rob Err 0.3828 (0.3709)  Uns 28.9 (29.3)  Dead 50.0 (49.6)  Alive 113.0 (113.0)  Tightness 28.90625 (29.34314)  Bias -4.13627 (-4.32454)  Diff 1.84708 (1.79856)  R 22.347  beta 0.480 (0.480)  kappa 1.000 (1.000)  \n",
      "[32: 100]: eps 0.157128  Time 0.035 (0.037)  Total Loss 1.3110 (1.3254)  L1 Loss 0.0000 (0.0000)  CE 0.4731 (0.4442)  RCE 1.3110 (1.3254)  Err 0.1289 (0.1209)  Rob Err 0.3594 (0.3728)  Uns 29.6 (29.4)  Dead 50.1 (49.6)  Alive 112.4 (112.9)  Tightness 29.58594 (29.43065)  Bias -3.54656 (-4.28044)  Diff 1.55059 (1.76244)  R 22.172  beta 0.476 (0.476)  kappa 1.000 (1.000)  \n",
      "[32: 150]: eps 0.158191  Time 0.032 (0.036)  Total Loss 1.3091 (1.3284)  L1 Loss 0.0000 (0.0000)  CE 0.4436 (0.4457)  RCE 1.3091 (1.3284)  Err 0.1016 (0.1212)  Rob Err 0.3867 (0.3752)  Uns 29.1 (29.5)  Dead 50.1 (49.6)  Alive 112.8 (112.9)  Tightness 29.13281 (29.46492)  Bias -3.58784 (-4.28688)  Diff 1.84477 (1.73039)  R 25.693  beta 0.473 (0.473)  kappa 1.000 (1.000)  \n",
      "[32: 200]: eps 0.159255  Time 0.032 (0.035)  Total Loss 1.4881 (1.3254)  L1 Loss 0.0000 (0.0000)  CE 0.5548 (0.4452)  RCE 1.4881 (1.3254)  Err 0.1562 (0.1218)  Rob Err 0.3945 (0.3752)  Uns 30.7 (29.5)  Dead 49.4 (49.6)  Alive 112.0 (112.8)  Tightness 30.66797 (29.54855)  Bias -4.07878 (-4.34674)  Diff 1.36636 (1.68288)  R 23.325  beta 0.469 (0.469)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:32 eps:0.1600]: Time 0.017 (0.035)  Total Loss 1.2161 (1.3282)  L1 Loss 0.0000 (0.0000)  CE 0.4145 (0.4476)  RCE 1.2161 (1.3282)  Uns 29.510 (29.579)  Dead 50.1 (49.6)  Alive 112.4 (112.8)  Tight 29.51042 (29.57933)  Bias -4.64543 (-4.36063)  Diff 1.28315 (1.65179)  Err 0.1146 (0.1223)  Rob Err 0.3542 (0.3757)  R 19.256  beta 0.467 (0.467)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 12.932199478149414\n",
      "layer 3 norm 9.821168899536133\n",
      "layer 5 norm 8.246379852294922\n",
      "Epoch time: 8.3650, Total time: 287.3754\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:32 eps:0.1600]: Time 0.003 (0.020)  Total Loss 1.1651 (1.2886)  L1 Loss 0.0000 (0.0000)  CE 0.3260 (0.4351)  RCE 1.1651 (1.2886)  Uns 26.438 (29.973)  Dead 52.7 (49.6)  Alive 112.9 (112.4)  Tight 26.43750 (29.97320)  Bias -2.01199 (-4.40258)  Diff 1.28708 (1.49107)  Err 0.0625 (0.1174)  Rob Err 0.2500 (0.3637)  R 17.576  beta 0.467 (0.467)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 12.932199478149414\n",
      "layer 3 norm 9.821168899536133\n",
      "layer 5 norm 8.246379852294922\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 33, learning rate [0.0005], epsilon 0.16 - 0.165\n",
      "[33:   0]: eps 0.160000  Time 0.136 (0.136)  Total Loss 1.3703 (1.3703)  L1 Loss 0.0000 (0.0000)  CE 0.4822 (0.4822)  RCE 1.3703 (1.3703)  Err 0.1484 (0.1484)  Rob Err 0.3750 (0.3750)  Uns 29.9 (29.9)  Dead 49.4 (49.4)  Alive 112.7 (112.7)  Tightness 29.88281 (29.88281)  Bias -3.31573 (-3.31573)  Diff 1.60287 (1.60287)  R 23.481  beta 0.467 (0.467)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33:  50]: eps 0.161064  Time 0.035 (0.047)  Total Loss 1.4106 (1.3559)  L1 Loss 0.0000 (0.0000)  CE 0.4625 (0.4688)  RCE 1.4106 (1.3559)  Err 0.0977 (0.1285)  Rob Err 0.4297 (0.3833)  Uns 30.2 (29.9)  Dead 49.5 (49.7)  Alive 112.2 (112.4)  Tightness 30.21484 (29.89844)  Bias -5.55657 (-4.46115)  Diff 1.03416 (1.39777)  R 23.708  beta 0.463 (0.463)  kappa 1.000 (1.000)  \n",
      "[33: 100]: eps 0.162128  Time 0.037 (0.042)  Total Loss 1.2782 (1.3524)  L1 Loss 0.0000 (0.0000)  CE 0.4336 (0.4713)  RCE 1.2782 (1.3524)  Err 0.1094 (0.1286)  Rob Err 0.3594 (0.3815)  Uns 30.7 (29.8)  Dead 48.8 (49.7)  Alive 112.5 (112.5)  Tightness 30.66797 (29.84054)  Bias -3.89610 (-4.46528)  Diff 1.22135 (1.33788)  R 22.825  beta 0.460 (0.460)  kappa 1.000 (1.000)  \n",
      "[33: 150]: eps 0.163191  Time 0.036 (0.042)  Total Loss 1.2940 (1.3453)  L1 Loss 0.0000 (0.0000)  CE 0.4789 (0.4695)  RCE 1.2940 (1.3453)  Err 0.1289 (0.1280)  Rob Err 0.3281 (0.3792)  Uns 30.3 (29.8)  Dead 49.2 (49.7)  Alive 112.4 (112.5)  Tightness 30.32812 (29.83154)  Bias -3.48988 (-4.40613)  Diff 1.22031 (1.29328)  R 24.174  beta 0.456 (0.456)  kappa 1.000 (1.000)  \n",
      "[33: 200]: eps 0.164255  Time 0.037 (0.041)  Total Loss 1.3827 (1.3401)  L1 Loss 0.0000 (0.0000)  CE 0.4620 (0.4666)  RCE 1.3827 (1.3401)  Err 0.1250 (0.1274)  Rob Err 0.4023 (0.3776)  Uns 30.8 (29.9)  Dead 48.6 (49.7)  Alive 112.7 (112.5)  Tightness 30.77344 (29.86320)  Bias -5.55670 (-4.46885)  Diff 1.04193 (1.24687)  R 23.723  beta 0.452 (0.452)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:33 eps:0.1650]: Time 0.017 (0.041)  Total Loss 1.3463 (1.3383)  L1 Loss 0.0000 (0.0000)  CE 0.4798 (0.4662)  RCE 1.3463 (1.3383)  Uns 30.375 (29.891)  Dead 49.5 (49.6)  Alive 112.1 (112.5)  Tight 30.37500 (29.89143)  Bias -4.30527 (-4.46160)  Diff 0.82452 (1.21681)  Err 0.1042 (0.1275)  Rob Err 0.3229 (0.3771)  R 22.296  beta 0.450 (0.450)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 12.309072494506836\n",
      "layer 3 norm 10.227336883544922\n",
      "layer 5 norm 8.336398124694824\n",
      "Epoch time: 9.7367, Total time: 297.1121\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:33 eps:0.1650]: Time 0.003 (0.020)  Total Loss 1.2021 (1.2984)  L1 Loss 0.0000 (0.0000)  CE 0.3628 (0.4522)  RCE 1.2021 (1.2984)  Uns 26.625 (29.730)  Dead 52.4 (49.9)  Alive 113.0 (112.4)  Tight 26.62500 (29.72970)  Bias -1.66851 (-4.40092)  Diff 0.58324 (1.06659)  Err 0.0625 (0.1234)  Rob Err 0.3125 (0.3685)  R 18.328  beta 0.450 (0.450)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 12.309072494506836\n",
      "layer 3 norm 10.227336883544922\n",
      "layer 5 norm 8.336398124694824\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 34, learning rate [0.0005], epsilon 0.165 - 0.17\n",
      "[34:   0]: eps 0.165000  Time 0.125 (0.125)  Total Loss 1.4808 (1.4808)  L1 Loss 0.0000 (0.0000)  CE 0.5812 (0.5812)  RCE 1.4808 (1.4808)  Err 0.1680 (0.1680)  Rob Err 0.4102 (0.4102)  Uns 30.6 (30.6)  Dead 48.9 (48.9)  Alive 112.5 (112.5)  Tightness 30.64844 (30.64844)  Bias -4.13471 (-4.13471)  Diff 1.20273 (1.20273)  R 23.916  beta 0.450 (0.450)  kappa 1.000 (1.000)  \n",
      "[34:  50]: eps 0.166064  Time 0.036 (0.042)  Total Loss 1.4290 (1.3597)  L1 Loss 0.0000 (0.0000)  CE 0.5058 (0.4826)  RCE 1.4290 (1.3597)  Err 0.1523 (0.1332)  Rob Err 0.4609 (0.3879)  Uns 30.0 (29.9)  Dead 49.9 (49.7)  Alive 112.0 (112.3)  Tightness 30.00391 (29.94156)  Bias -5.96738 (-4.72529)  Diff 0.69502 (1.01378)  R 24.610  beta 0.446 (0.446)  kappa 1.000 (1.000)  \n",
      "[34: 100]: eps 0.167128  Time 0.043 (0.040)  Total Loss 1.2465 (1.3507)  L1 Loss 0.0000 (0.0000)  CE 0.4500 (0.4800)  RCE 1.2465 (1.3507)  Err 0.1211 (0.1311)  Rob Err 0.3398 (0.3829)  Uns 30.1 (30.0)  Dead 48.7 (49.7)  Alive 113.2 (112.3)  Tightness 30.11719 (29.96380)  Bias -3.94320 (-4.55941)  Diff 0.85705 (0.96534)  R 25.074  beta 0.443 (0.443)  kappa 1.000 (1.000)  \n",
      "[34: 150]: eps 0.168191  Time 0.038 (0.041)  Total Loss 1.0704 (1.3461)  L1 Loss 0.0000 (0.0000)  CE 0.3417 (0.4793)  RCE 1.0704 (1.3461)  Err 0.0820 (0.1320)  Rob Err 0.3086 (0.3803)  Uns 29.4 (29.9)  Dead 50.6 (49.7)  Alive 112.0 (112.4)  Tightness 29.38672 (29.94130)  Bias -3.17444 (-4.50231)  Diff 0.71341 (0.91810)  R 25.639  beta 0.439 (0.439)  kappa 1.000 (1.000)  \n",
      "[34: 200]: eps 0.169255  Time 0.053 (0.041)  Total Loss 1.4242 (1.3449)  L1 Loss 0.0000 (0.0000)  CE 0.5408 (0.4802)  RCE 1.4242 (1.3449)  Err 0.1562 (0.1317)  Rob Err 0.4375 (0.3792)  Uns 30.6 (30.0)  Dead 49.0 (49.7)  Alive 112.3 (112.3)  Tightness 30.63672 (29.97334)  Bias -5.37120 (-4.45027)  Diff 0.80936 (0.88329)  R 24.246  beta 0.436 (0.436)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:34 eps:0.1700]: Time 0.018 (0.040)  Total Loss 1.3959 (1.3478)  L1 Loss 0.0000 (0.0000)  CE 0.5585 (0.4828)  RCE 1.3959 (1.3478)  Uns 30.292 (30.002)  Dead 50.1 (49.7)  Alive 111.6 (112.3)  Tight 30.29167 (30.00173)  Bias -5.60036 (-4.47356)  Diff 0.54017 (0.85524)  Err 0.1979 (0.1325)  Rob Err 0.3854 (0.3802)  R 20.508  beta 0.433 (0.433)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 10.672401428222656\n",
      "layer 5 norm 8.393745422363281\n",
      "Epoch time: 9.5900, Total time: 306.7021\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:34 eps:0.1700]: Time 0.004 (0.022)  Total Loss 1.2501 (1.3068)  L1 Loss 0.0000 (0.0000)  CE 0.3938 (0.4684)  RCE 1.2501 (1.3068)  Uns 27.562 (30.673)  Dead 52.3 (49.4)  Alive 112.1 (111.9)  Tight 27.56250 (30.67290)  Bias -1.48046 (-4.34052)  Diff 0.22476 (0.75018)  Err 0.1250 (0.1260)  Rob Err 0.3125 (0.3687)  R 18.552  beta 0.433 (0.433)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 10.672401428222656\n",
      "layer 5 norm 8.393745422363281\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 35, learning rate [0.0005], epsilon 0.17 - 0.175\n",
      "[35:   0]: eps 0.170000  Time 0.138 (0.138)  Total Loss 1.3243 (1.3243)  L1 Loss 0.0000 (0.0000)  CE 0.5028 (0.5028)  RCE 1.3243 (1.3243)  Err 0.1406 (0.1406)  Rob Err 0.3633 (0.3633)  Uns 31.1 (31.1)  Dead 49.3 (49.3)  Alive 111.7 (111.7)  Tightness 31.08594 (31.08594)  Bias -3.97739 (-3.97739)  Diff 0.71552 (0.71552)  R 23.673  beta 0.433 (0.433)  kappa 1.000 (1.000)  \n",
      "[35:  50]: eps 0.171064  Time 0.053 (0.048)  Total Loss 1.5078 (1.3607)  L1 Loss 0.0000 (0.0000)  CE 0.5524 (0.4946)  RCE 1.5078 (1.3607)  Err 0.1523 (0.1348)  Rob Err 0.4062 (0.3833)  Uns 29.9 (30.3)  Dead 50.2 (49.8)  Alive 111.9 (111.9)  Tightness 29.89844 (30.28692)  Bias -4.55602 (-4.34917)  Diff 0.70489 (0.67685)  R 24.731  beta 0.430 (0.430)  kappa 1.000 (1.000)  \n",
      "[35: 100]: eps 0.172128  Time 0.031 (0.042)  Total Loss 1.3424 (1.3557)  L1 Loss 0.0000 (0.0000)  CE 0.4948 (0.4955)  RCE 1.3424 (1.3557)  Err 0.1328 (0.1351)  Rob Err 0.3516 (0.3808)  Uns 30.8 (30.2)  Dead 49.5 (49.9)  Alive 111.7 (111.9)  Tightness 30.83594 (30.22266)  Bias -5.18914 (-4.37302)  Diff 0.62434 (0.61148)  R 24.292  beta 0.426 (0.426)  kappa 1.000 (1.000)  \n",
      "[35: 150]: eps 0.173191  Time 0.033 (0.039)  Total Loss 1.3661 (1.3542)  L1 Loss 0.0000 (0.0000)  CE 0.5502 (0.4963)  RCE 1.3661 (1.3542)  Err 0.1328 (0.1351)  Rob Err 0.3906 (0.3822)  Uns 30.2 (30.3)  Dead 49.4 (49.8)  Alive 112.4 (111.9)  Tightness 30.19141 (30.27238)  Bias -3.98610 (-4.32045)  Diff 0.53221 (0.57552)  R 26.460  beta 0.423 (0.423)  kappa 1.000 (1.000)  \n",
      "[35: 200]: eps 0.174255  Time 0.032 (0.038)  Total Loss 1.3613 (1.3566)  L1 Loss 0.0000 (0.0000)  CE 0.5521 (0.4990)  RCE 1.3613 (1.3566)  Err 0.1445 (0.1357)  Rob Err 0.3555 (0.3818)  Uns 30.2 (30.3)  Dead 50.0 (49.8)  Alive 111.9 (111.9)  Tightness 30.15625 (30.29960)  Bias -4.24113 (-4.28301)  Diff 0.50526 (0.53979)  R 25.564  beta 0.419 (0.419)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:35 eps:0.1750]: Time 0.016 (0.037)  Total Loss 1.5305 (1.3547)  L1 Loss 0.0000 (0.0000)  CE 0.5368 (0.4987)  RCE 1.5305 (1.3547)  Uns 31.156 (30.317)  Dead 49.9 (49.8)  Alive 111.0 (111.9)  Tight 31.15625 (30.31707)  Bias -5.90442 (-4.27141)  Diff 0.28085 (0.52475)  Err 0.1562 (0.1356)  Rob Err 0.4271 (0.3814)  R 21.666  beta 0.417 (0.417)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 11.134563446044922\n",
      "layer 5 norm 8.444459915161133\n",
      "Epoch time: 9.0135, Total time: 315.7156\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:35 eps:0.1750]: Time 0.003 (0.017)  Total Loss 1.2822 (1.3124)  L1 Loss 0.0000 (0.0000)  CE 0.4428 (0.4882)  RCE 1.2822 (1.3124)  Uns 27.188 (29.984)  Dead 52.2 (50.2)  Alive 112.6 (111.8)  Tight 27.18750 (29.98370)  Bias -1.52701 (-4.02387)  Diff -0.08398 (0.42921)  Err 0.1250 (0.1315)  Rob Err 0.3125 (0.3689)  R 18.352  beta 0.417 (0.417)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 11.134563446044922\n",
      "layer 5 norm 8.444459915161133\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 36, learning rate [0.0005], epsilon 0.175 - 0.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36:   0]: eps 0.175000  Time 0.093 (0.093)  Total Loss 1.3219 (1.3219)  L1 Loss 0.0000 (0.0000)  CE 0.5013 (0.5013)  RCE 1.3219 (1.3219)  Err 0.1133 (0.1133)  Rob Err 0.3828 (0.3828)  Uns 30.6 (30.6)  Dead 49.9 (49.9)  Alive 111.4 (111.4)  Tightness 30.64062 (30.64062)  Bias -4.80244 (-4.80244)  Diff 0.29515 (0.29515)  R 23.066  beta 0.417 (0.417)  kappa 1.000 (1.000)  \n",
      "[36:  50]: eps 0.176064  Time 0.033 (0.040)  Total Loss 1.3767 (1.3472)  L1 Loss 0.0000 (0.0000)  CE 0.5103 (0.5051)  RCE 1.3767 (1.3472)  Err 0.1289 (0.1340)  Rob Err 0.3867 (0.3796)  Uns 30.9 (30.4)  Dead 49.5 (49.8)  Alive 111.7 (111.8)  Tightness 30.85156 (30.43137)  Bias -5.31030 (-4.01451)  Diff 0.42239 (0.38329)  R 24.878  beta 0.413 (0.413)  kappa 1.000 (1.000)  \n",
      "[36: 100]: eps 0.177128  Time 0.040 (0.037)  Total Loss 1.3872 (1.3502)  L1 Loss 0.0000 (0.0000)  CE 0.5460 (0.5078)  RCE 1.3872 (1.3502)  Err 0.1523 (0.1360)  Rob Err 0.3711 (0.3814)  Uns 31.6 (30.4)  Dead 48.5 (49.8)  Alive 111.9 (111.8)  Tightness 31.55469 (30.43823)  Bias -4.13144 (-4.04019)  Diff 0.44787 (0.35645)  R 26.534  beta 0.410 (0.410)  kappa 1.000 (1.000)  \n",
      "[36: 150]: eps 0.178191  Time 0.032 (0.037)  Total Loss 1.2323 (1.3547)  L1 Loss 0.0000 (0.0000)  CE 0.4493 (0.5092)  RCE 1.2323 (1.3547)  Err 0.1172 (0.1371)  Rob Err 0.3359 (0.3839)  Uns 30.3 (30.5)  Dead 50.1 (49.8)  Alive 111.6 (111.7)  Tightness 30.26562 (30.47010)  Bias -3.20953 (-3.95463)  Diff 0.27828 (0.32293)  R 25.199  beta 0.406 (0.406)  kappa 1.000 (1.000)  \n",
      "[36: 200]: eps 0.179255  Time 0.035 (0.037)  Total Loss 1.5190 (1.3600)  L1 Loss 0.0000 (0.0000)  CE 0.5997 (0.5135)  RCE 1.5190 (1.3600)  Err 0.1641 (0.1387)  Rob Err 0.4453 (0.3845)  Uns 32.4 (30.6)  Dead 48.5 (49.8)  Alive 111.2 (111.6)  Tightness 32.35938 (30.55059)  Bias -4.60216 (-3.96592)  Diff 0.12612 (0.28779)  R 24.300  beta 0.402 (0.402)  kappa 1.000 (1.000)  \n",
      "[FINAL RESULT epoch:36 eps:0.1800]: Time 0.020 (0.037)  Total Loss 1.4160 (1.3611)  L1 Loss 0.0000 (0.0000)  CE 0.5837 (0.5152)  RCE 1.4160 (1.3611)  Uns 31.729 (30.566)  Dead 49.2 (49.8)  Alive 111.0 (111.6)  Tight 31.72917 (30.56587)  Bias -3.33588 (-3.92525)  Diff 0.38633 (0.27151)  Err 0.1562 (0.1393)  Rob Err 0.3958 (0.3842)  R 24.394  beta 0.400 (0.400)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 11.620996475219727\n",
      "layer 5 norm 8.504626274108887\n",
      "Epoch time: 8.8103, Total time: 324.5259\n",
      "Evaluating...\n",
      "[FINAL RESULT epoch:36 eps:0.1800]: Time 0.003 (0.018)  Total Loss 1.2828 (1.3189)  L1 Loss 0.0000 (0.0000)  CE 0.4576 (0.5041)  RCE 1.2828 (1.3189)  Uns 27.938 (29.888)  Dead 51.5 (50.6)  Alive 112.6 (111.5)  Tight 27.93750 (29.88770)  Bias -0.63789 (-3.66498)  Diff -0.46444 (0.19080)  Err 0.1250 (0.1347)  Rob Err 0.3125 (0.3730)  R 18.268  beta 0.400 (0.400)  kappa 1.000 (1.000)  \n",
      "\n",
      "layer 1 norm 11.730475425720215\n",
      "layer 3 norm 11.620996475219727\n",
      "layer 5 norm 8.504626274108887\n",
      "saving to ./mnist_crown/regular_dense.pth\n",
      "Epoch 37, learning rate [0.0005], epsilon 0.18 - 0.185\n",
      "[37:   0]: eps 0.180000  Time 0.108 (0.108)  Total Loss 1.2604 (1.2604)  L1 Loss 0.0000 (0.0000)  CE 0.4607 (0.4607)  RCE 1.2604 (1.2604)  Err 0.0977 (0.0977)  Rob Err 0.3359 (0.3359)  Uns 29.7 (29.7)  Dead 51.3 (51.3)  Alive 111.0 (111.0)  Tightness 29.67969 (29.67969)  Bias -2.90437 (-2.90437)  Diff 0.24495 (0.24495)  R 25.847  beta 0.400 (0.400)  kappa 1.000 (1.000)  \n",
      "[37:  50]: eps 0.181064  Time 0.034 (0.041)  Total Loss 1.5395 (1.3559)  L1 Loss 0.0000 (0.0000)  CE 0.6055 (0.5184)  RCE 1.5395 (1.3559)  Err 0.1914 (0.1383)  Rob Err 0.4570 (0.3823)  Uns 30.5 (30.6)  Dead 50.4 (50.0)  Alive 111.1 (111.4)  Tightness 30.50391 (30.62737)  Bias -2.67022 (-3.68288)  Diff 0.16928 (0.11702)  R 25.555  beta 0.396 (0.396)  kappa 1.000 (1.000)  \n",
      "[37: 100]: eps 0.182128  Time 0.033 (0.038)  Total Loss 1.4209 (1.3672)  L1 Loss 0.0000 (0.0000)  CE 0.5241 (0.5282)  RCE 1.4209 (1.3672)  Err 0.1602 (0.1421)  Rob Err 0.3867 (0.3859)  Uns 30.4 (30.6)  Dead 50.4 (50.0)  Alive 111.2 (111.4)  Tightness 30.40234 (30.57886)  Bias -3.38482 (-3.64039)  Diff 0.13755 (0.09603)  R 30.123  beta 0.393 (0.393)  kappa 1.000 (1.000)  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b3f47c9a9b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/azizilab/attacks/CROWN-IBP/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with torch.autograd.detect_anomaly():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_eps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmethod_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlr_decay_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0;31m# Use stepLR. Note that we manually set up epoch number here, so the +1 offset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/azizilab/attacks/CROWN-IBP/train.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(model, t, loader, eps_scheduler, max_eps, norm, logger, verbose, train, opt, method, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eps {} close to 0, using natural training'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_eps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"natural\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_multiplier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/scvi/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_filename\u001b[0;34m(cls, manager, handle, size)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train import main\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
